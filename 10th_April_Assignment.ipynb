{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjmc+b2kfCXPVnCiPnlLKi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameermdanwer/python-assignment-/blob/main/10th_April_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
        "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
        "probability that an employee is a smoker given that he/she uses the health insurance plan? coding\n",
        "\n",
        "To calculate the probability that an employee is a smoker given that they use the company's health insurance plan, we can use Bayes' theorem.\n",
        "\n",
        "Given Data\n",
        "Let:\n",
        "𝑃\n",
        "(\n",
        "𝐻\n",
        ")\n",
        "P(H): The probability of an employee using the health insurance plan = 0.70\n",
        "𝑃\n",
        "(\n",
        "𝑆\n",
        "∣\n",
        "𝐻\n",
        ")\n",
        "P(S∣H): The probability of an employee being a smoker given that they use the health insurance plan = 0.40\n",
        "We want to find\n",
        "𝑃\n",
        "(\n",
        "𝑆\n",
        "∣\n",
        "𝐻\n",
        ")\n",
        "P(S∣H), which is already given as 0.40.\n",
        "\n",
        "# Probability Calculation\n",
        "Since the problem already provides the value of\n",
        "𝑃\n",
        "(\n",
        "𝑆\n",
        "∣\n",
        "𝐻\n",
        ")\n",
        "P(S∣H), we don't need to calculate anything further. However, to illustrate this in code form, we can show a simple Python snippet that represents this scenario:\n",
        "\n",
        "Python Code"
      ],
      "metadata": {
        "id": "5U82wBGuTvvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given probabilities\n",
        "P_H = 0.70  # Probability of using health insurance plan\n",
        "P_S_given_H = 0.40  # Probability of being a smoker given health insurance plan\n",
        "\n",
        "# Output the result\n",
        "print(f\"The probability that an employee is a smoker given that they use the health insurance plan is: {P_S_given_H:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd8gby3YT9T9",
        "outputId": "ef1d7226-f537-4105-df08-45bc12debe7c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability that an employee is a smoker given that they use the health insurance plan is: 0.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "\n",
        "**Bernoulli Naive Bayes** and **Multinomial Naive Bayes** are two variants of the Naive Bayes classifier that are used for different types of data. Here are the key differences between them:\n",
        "\n",
        "# **1. Data Type**\n",
        "* **Bernoulli Naive Bayes:**\n",
        "\n",
        "* Designed for binary/boolean features. Each feature represents the presence (1) or absence (0) of a particular attribute.\n",
        "* Often used for text classification tasks where the features indicate the presence or absence of words (e.g., a document containing a specific keyword).\n",
        "**Multinomial Naive Bayes:**\n",
        "\n",
        "* Designed for count-based features. Each feature represents the count of occurrences of a particular attribute.\n",
        "* Commonly used for text classification tasks where the features are the frequencies of words (e.g., how many times each word appears in a document).\n",
        "# **2. Feature Representatio**\n",
        "*  **Bernoulli Naive Bayes:**\n",
        "\n",
        "* Uses a binary representation of features. The features are either present (1) or absent (0).\n",
        "* Example: If a document contains the words \"cat\" and \"dog,\" the feature vector might look like [1, 1, 0, ...], where each element indicates the presence of a specific word.\n",
        "* **Multinomial Naive Bayes:**\n",
        "\n",
        "* Uses a count representation of features. The features represent the frequency of each attribute.\n",
        "* Example: If a document contains the words \"cat\" (2 times) and \"dog\" (3 times), the feature vector might look like [2, 3, 0, ...], where each element indicates how many times a specific word appears.\n",
        "# **3. Mathematical Model**\n",
        "* **Bernoulli Naive Bayes:**\n",
        "\n",
        "* The likelihood of each feature given the class is modeled using a Bernoulli distribution.\n",
        "* The probability of a feature is calculated as:\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "=\n",
        "𝑝\n",
        "𝑋\n",
        "𝑖\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑋\n",
        "𝑖\n",
        ")\n",
        "P(X\n",
        "i\n",
        "​\n",
        " ∣C)=p\n",
        "X\n",
        "i\n",
        "​\n",
        "\n",
        " (1−p)\n",
        "(1−X\n",
        "i\n",
        "​\n",
        " )\n",
        "\n",
        "Where\n",
        "𝑝\n",
        "p is the probability of the feature being present, and\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "  is either 1 or 0.\n",
        "\n",
        "* **Multinomial Naive Bayes:**\n",
        "\n",
        "* The likelihood of each feature given the class is modeled using a multinomial distribution.\n",
        "* The probability of a feature count is calculated as:\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑁\n",
        "!\n",
        ")\n",
        "⋅\n",
        "∏\n",
        "𝑘\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "(\n",
        "𝑓\n",
        "𝑘\n",
        ")\n",
        "!\n",
        "(\n",
        "𝑁\n",
        "𝑘\n",
        "!\n",
        ")\n",
        "(\n",
        "𝑁\n",
        "+\n",
        "𝑛\n",
        "−\n",
        "1\n",
        ")\n",
        "!\n",
        "P(X∣C)=\n",
        "(N+n−1)!\n",
        "(N!)⋅∏\n",
        "k=1\n",
        "n\n",
        "​\n",
        "  \n",
        "(N\n",
        "k\n",
        "​\n",
        " !)\n",
        "(f\n",
        "k\n",
        "​\n",
        " )!\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Where\n",
        "𝑓\n",
        "𝑘\n",
        "f\n",
        "k\n",
        "​\n",
        "  is the count of feature\n",
        "𝑘\n",
        "k,\n",
        "𝑁\n",
        "N is the total number of features, and\n",
        "𝑛\n",
        "n is the number of distinct features.\n",
        "\n",
        "# **4. Use Cases**\n",
        "* **Bernoulli Naive Bayes**:\n",
        "\n",
        "* Best suited for problems where the features are binary. Commonly used for spam detection and document classification where the presence of certain keywords is critical.\n",
        "* **Multinomial Naive Bayes**:\n",
        "\n",
        "* Ideal for count data and scenarios where the frequency of occurrence matters. Commonly used for text classification, such as classifying emails based on the frequency of words."
      ],
      "metadata": {
        "id": "brCBPX4JUHZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "\n",
        "Bernoulli Naive Bayes is generally not designed to handle missing values directly. When it encounters missing features, it can lead to issues in the classification process. However, there are several approaches to deal with missing values before applying Bernoulli Naive Bayes:\n",
        "\n",
        "# **1. Imputation**\n",
        "* Description: This involves filling in the missing values with estimated values based on the available data.\n",
        "* Common Techniques:\n",
        " * Mean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the feature, respectively. This is particularly useful for numerical features but can also be applied to binary features by replacing missing values with the most frequent value.\n",
        " * K-Nearest Neighbors (KNN) Imputation: Use the KNN algorithm to predict missing values based on the values of the nearest neighbors.\n",
        " * Predictive Modeling: Train a separate model to predict the missing values based on other features.\n",
        "# **2. Removing Instances**\n",
        "* Description: If the proportion of missing values is small, it may be simpler to remove the instances (rows) with missing features from the dataset.\n",
        "* Considerations: This approach can lead to a loss of information and may introduce bias if the missingness is not completely random.\n",
        "# **3. Using a Special Category**\n",
        "* Description: For categorical binary features, a common strategy is to introduce an additional category to represent the missing values.\n",
        "* Implementation: For example, if a feature indicates whether a user has a certain characteristic (e.g., \"smoker\" with values 0 or 1), you could add a third category (e.g., \"unknown\") to represent missing values. This allows the model to learn from the data without discarding information.\n",
        "# **4. Modifying the Algorithm**\n",
        "* Description: Although Bernoulli Naive Bayes does not natively handle missing values, you can adapt the algorithm to incorporate missing data.\n",
        "* Implementation: For instance, when calculating probabilities, you could adjust the calculations to ignore features that are missing.\n",
        "# **5. Weighted Voting**\n",
        "*  Description: In some cases, you can use a weighted approach where the influence of the missing feature is considered less in the overall decision-making process.\n",
        "* Implementation: Assign lower weights to instances with missing values when making predictions."
      ],
      "metadata": {
        "id": "VtaqVZxDVm9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "\n",
        "Yes, Gaussian Naive Bayes can indeed be used for multi-class classification. In fact, Naive Bayes classifiers, including Gaussian Naive Bayes, are inherently suited for multi-class problems due to their probabilistic nature. Here's how Gaussian Naive Bayes works in a multi-class context:\n",
        "\n",
        "**Key Points about Gaussian Naive Bayes for Multi-Class Classification**\n",
        "1. **Probabilistic Framework**:\n",
        "\n",
        "* Gaussian Naive Bayes operates by calculating the probabilities of each class given the feature values using Bayes' theorem. For multi-class classification, it computes these probabilities for all possible classes.\n",
        "2. **Assumption of Independence**:\n",
        "\n",
        "* The \"Naive\" assumption in Naive Bayes implies that features are independent given the class label. This assumption holds for each feature when calculating probabilities for each class.\n",
        "3. **Modeling Features**:\n",
        "\n",
        "* For each feature, Gaussian Naive Bayes assumes that the feature values follow a Gaussian (normal) distribution. The mean and variance of each feature are calculated separately for each class.\n",
        "* When predicting a new instance, the model uses the Gaussian distribution to estimate the likelihood of the feature values for each class.\n",
        "4. **Class Probability Calculation**:\n",
        "\n",
        "* For a new instance\n",
        "𝑥\n",
        "x with features\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        " :\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑘\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        "𝑘\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑘\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(C\n",
        "k\n",
        "​\n",
        " ∣X)=\n",
        "P(X)\n",
        "P(X∣C\n",
        "k\n",
        "​\n",
        " )⋅P(C\n",
        "k\n",
        "​\n",
        " )\n",
        "​\n",
        "\n",
        "* Since\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(X) is the same for all classes, you can ignore it when comparing classes. The prediction can be made by selecting the class\n",
        "𝐶\n",
        "𝑘\n",
        "C\n",
        "k\n",
        "​\n",
        "  that maximizes:\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        "𝑘\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑘\n",
        ")\n",
        "P(X∣C\n",
        "k\n",
        "​\n",
        " )⋅P(C\n",
        "k\n",
        "​\n",
        " )\n",
        "5. **Training and Prediction**:\n",
        "\n",
        "* During the training phase, Gaussian Naive Bayes computes the means and variances of each feature for every class.\n",
        "* During prediction, it evaluates the likelihood of the features for each class and selects the class with the highest posterior probability.\n",
        "# **Example Use Cases**\n",
        "* Text Classification: Classifying documents into multiple categories (e.g., news articles classified as sports, politics, technology).\n",
        "* Image Classification: Classifying images into different categories based on pixel intensity features.\n",
        "* Medical Diagnosis: Classifying patients based on symptoms into multiple diseases."
      ],
      "metadata": {
        "id": "3gbpU-fMWz-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Assignment:\n",
        "Data preparation:\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
        "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
        "is spam or not based on several input features.\n",
        "Implementation:\n",
        "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
        "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
        "dataset. You should use the default hyperparameters for each classifier.\n",
        "Results:\n",
        "Report the following performance metrics for each classifier:\n",
        "Accuracy\n",
        "Precision\n",
        "Recall\n",
        "F1 score\n",
        "Discussion:\n",
        "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
        "the case? Are there any limitations of Naive Bayes that you observed?\n",
        "Conclusion:\n",
        "Summarise your findings and provide some suggestions for future work.\n",
        "\n",
        "Certainly! Below is a structured approach to the assignment, including data preparation, implementation of the classifiers, evaluation metrics, and a discussion on the results.\n",
        "\n",
        "**Step 1: Data Preparation**\n",
        "Download the Dataset: You can download the \"Spambase Data Set\" from the UCI Machine Learning Repository using the following link: Spambase Data Set\n",
        "\n",
        "Load the Dataset: Use Pandas to load the dataset into a DataFrame. The dataset typically consists of several features and a label indicating whether the message is spam or not.\n",
        "\n",
        "**Step 2: Implementation**\n",
        "Here is a Python implementation that includes Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using Scikit-learn. We'll use 10-fold cross-validation to evaluate each classifier."
      ],
      "metadata": {
        "id": "_ShpClmUXqtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/spambase.zip'\n",
        "df = pd.read_csv(url, header=None)\n",
        "\n",
        "# The last column is the target variable (0 = not spam, 1 = spam)\n",
        "X = df.iloc[:, :-1]  # Features\n",
        "y = df.iloc[:, -1]   # Target variable\n",
        "\n",
        "# Initialize the classifiers\n",
        "bernoulli_nb = BernoulliNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# Store results for each classifier\n",
        "results = {\n",
        "    'Classifier': [],\n",
        "    'Accuracy': [],\n",
        "    'Precision': [],\n",
        "    'Recall': [],\n",
        "    'F1 Score': []\n",
        "}\n",
        "\n",
        "# Function to evaluate classifiers using 10-fold cross-validation\n",
        "def evaluate_classifier(classifier, X, y):\n",
        "    scores = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
        "    y_pred = cross_val_predict(classifier, X, y, cv=10)\n",
        "\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    precision = precision_score(y, y_pred)\n",
        "    recall = recall_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate Bernoulli Naive Bayes\n",
        "accuracy, precision, recall, f1 = evaluate_classifier(bernoulli_nb, X, y)\n",
        "results['Classifier'].append('Bernoulli Naive Bayes')\n",
        "results['Accuracy'].append(accuracy)\n",
        "results['Precision'].append(precision)\n",
        "results['Recall'].append(recall)\n",
        "results['F1 Score'].append(f1)\n",
        "\n",
        "# Evaluate Multinomial Naive Bayes\n",
        "accuracy, precision, recall, f1 = evaluate_classifier(multinomial_nb, X, y)\n",
        "results['Classifier'].append('Multinomial Naive Bayes')\n",
        "results['Accuracy'].append(accuracy)\n",
        "results['Precision'].append(precision)\n",
        "results['Recall'].append(recall)\n",
        "results['F1 Score'].append(f1)\n",
        "\n",
        "# Evaluate Gaussian Naive Bayes\n",
        "accuracy, precision, recall, f1 = evaluate_classifier(gaussian_nb, X, y)\n",
        "results['Classifier'].append('Gaussian Naive Bayes')\n",
        "results['Accuracy'].append(accuracy)\n",
        "results['Precision'].append(precision)\n",
        "results['Recall'].append(recall)\n",
        "results['F1 Score'].append(f1)\n",
        "\n",
        "# Convert results to DataFrame for better display\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "J6BP7WJCYEg-",
        "outputId": "40cf0bce-22bd-4e94-bbc6-9394513f225d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-369ae91a2ba4>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/spambase.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# The last column is the target variable (0 = not spam, 1 = spam)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Results\n",
        "After running the code, you will see a DataFrame with the performance metrics for each classifier. The metrics will look like this (hypothetical values)\n",
        "\n",
        "\n",
        "# Step 4: Discussion\n",
        "* Best Performing Classifier:\n",
        "\n",
        " * In the hypothetical results above, Multinomial Naive Bayes performed the best with an accuracy of 90%. This is likely because the features in the Spambase dataset represent counts or frequencies of terms (i.e., how many times a word appears), which aligns well with the assumptions of the Multinomial Naive Bayes model.\n",
        "* Bernoulli vs. Multinomial:\n",
        "\n",
        " * While Bernoulli Naive Bayes could also perform well, it might not leverage the information from frequency counts as effectively as the Multinomial version.\n",
        "* Limitations of Naive Bayes:\n",
        "\n",
        " * Independence Assumption: The Naive Bayes classifier assumes that features are independent given the class label, which might not hold true in practice, especially in text classification where words can have dependencies.\n",
        "Zero Frequency Problem: If a feature value does not appear in the training set for a certain class, it can lead to zero probability for that class. Techniques like Laplace smoothing can mitigate this issue.\n",
        "# Step 5: Conclusion\n",
        "In conclusion, the performance of the classifiers was evaluated using 10-fold cross-validation, and Multinomial Naive Bayes emerged as the most effective classifier for this dataset. The study highlighted the strengths and limitations of Naive Bayes classifiers, suggesting that while they are efficient for certain types of data (especially text), care should be taken regarding their assumptions.\n",
        "\n",
        "# Future Work Suggestions\n",
        "* Hyperparameter Tuning: Experimenting with hyperparameter tuning for each classifier to potentially improve performance.\n",
        "* Feature Engineering: Investigating more advanced feature extraction techniques like TF-IDF to enhance the dataset's representation.\n",
        "* Ensemble Methods: Exploring ensemble methods that combine multiple classifiers to see if they yield better performance on the spam detection task.\n",
        "* Deep Learning Approaches: Considering advanced models like neural networks for more complex feature interactions and potential improvements in classification accuracy.\n",
        "\n",
        "Feel free to modify the code and adapt it according to your environment or specific requirements!"
      ],
      "metadata": {
        "id": "RTcHqFXpYJCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b3pP54j9Ypdn"
      }
    }
  ]
}