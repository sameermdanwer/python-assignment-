{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1TqHzY+04qb/rsZKCKgvE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameermdanwer/python-assignment-/blob/main/Feature_Engineering_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
        "application.\n",
        "\n",
        "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform features to a common scale without distorting differences in the ranges of values. It is particularly useful when dealing with algorithms that rely on distance calculations, such as k-nearest neighbors, support vector machines, and neural networks.\n",
        "\n",
        "Definition\n",
        "Min-Max scaling rescales the feature values to a specific range, typically between 0 and 1. The formula for Min-Max scaling of a feature (x) is:\n",
        "\n",
        "[\n",
        "x' = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n",
        "]\n",
        "\n",
        "where:\n",
        "\n",
        "* (x) is the original value,\n",
        "* (x' ) is the scaled value,\n",
        "* (x_{\\text{min}}) is the minimum value of the feature,\n",
        "* (x_{\\text{max}}) is the maximum value of the feature.\n",
        "# Application\n",
        "Min-Max scaling is used when:\n",
        "\n",
        "* The features have different units or ranges.\n",
        "* You want to ensure all features contribute equally to the distance calculations.\n",
        "# Step 1: Calculate Min and Max Values\n",
        " * For Height:\n",
        "\n",
        "(x_{\\text{min}} = 150)\n",
        "(x_{\\text{max}} = 190)\n",
        "\n",
        " * For Weight:\n",
        "\n",
        "(y_{\\text{min}} = 50)\n",
        "(y_{\\text{max}} = 90)\n",
        "\n",
        "# Step 2: Apply Min-Max Scaling\n",
        "* Height Scaling:\n",
        "[\n",
        "\\begin{align*}\n",
        "\\text{For } 150: & \\quad \\frac{150 - 150}{190 - 150} = 0 \\\n",
        "\\text{For } 160: & \\quad \\frac{160 - 150}{190 - 150} = \\frac{10}{40} = 0.25 \\\n",
        "\\text{For } 170: & \\quad \\frac{170 - 150}{190 - 150} = \\frac{20}{40} = 0.5 \\\n",
        "\\text{For } 180: & \\quad \\frac{180 - 150}{190 - 150} = \\frac{30}{40} = 0.75 \\\n",
        "\\text{For } 190: & \\quad \\frac{190 - 150}{190 - 150} = 1 \\\n",
        "\\end{align*}\n",
        "]\n",
        "\n",
        "* Weight Scaling:\n",
        "[\n",
        "\\begin{align*}\n",
        "\\text{For } 50: & \\quad \\frac{50 - 50}{90 - 50} = 0 \\\n",
        "\\text{For } 60: & \\quad \\frac{60 - 50}{90 - 50} = \\frac{10}{40} = 0.25 \\\n",
        "\\text{For } 70: & \\quad \\frac{70 - 50}{90 - 50} = \\frac{20}{40} = 0.5 \\\n",
        "\\text{For } 80: & \\quad \\frac{80 - 50}{90 - 50} = \\frac{30}{40} = 0.75 \\\n",
        "\\text{For } 90: & \\quad \\frac{90 - 50}{90 - 50} = 1 \\\n",
        "\\end{align*}\n",
        "]"
      ],
      "metadata": {
        "id": "4uq9qUWCkde7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
        "Provide an example to illustrate its application.\n",
        "\n",
        "\n",
        "The Unit Vector technique, also known as vector normalization or length normalization, is a feature scaling technique that transforms data so that the magnitude (or length) of each feature vector is equal to one. This is particularly useful in scenarios where the direction of the data points is more critical than the magnitude. The Unit Vector technique is often used in machine learning algorithms that rely on the angle or the direction of the data points, such as cosine similarity.\n",
        "\n",
        "* Definition\n",
        "The Unit Vector normalization transforms a feature vector ( \\mathbf{x} ) into a unit vector ( \\mathbf{x}' ) using the following formula:\n",
        "\n",
        "[\n",
        "\\mathbf{x}' = \\frac{\\mathbf{x}}{|\\mathbf{x}|}\n",
        "]\n",
        "\n",
        "where ( |\\mathbf{x}| ) is the Euclidean norm (or length) of the vector, defined as:\n",
        "\n",
        "[\n",
        "|\\mathbf{x}| = \\sqrt{x_1^2 + x_2^2 + ... + x_n^2}\n",
        "]\n",
        "\n",
        "# Key Differences from Min-Max Scaling\n",
        "1. Purpose:\n",
        "\n",
        "* Min-Max Scaling rescales features to a fixed range (typically [0, 1]) and preserves the original data distribution.\n",
        "* Unit Vector Normalization focuses on re-scaling the length of the feature vector to 1 while preserving its direction.\n",
        "2. Output Range:\n",
        "\n",
        "* After Min-Max scaling, the features lie within a specified range.\n",
        "* After Unit Vector normalization, the features maintain their relative ratio but the overall scale is unified (the magnitude is 1).\n",
        "3. Use Cases:\n",
        "\n",
        "* Min-Max Scaling is most useful when working with algorithms sensitive to the scale of features, like k-nearest neighbors (KNN) and neural networks.\n",
        "* Unit Vector Normalization is particularly advantageous in high-dimensional spaces and applications involving angles between vectors (e.g., text classification with TF-IDF).\n",
        "\n",
        "Example\n",
        "\n",
        "Letâ€™s consider a simple feature vector from a dataset with three dimensions:\n",
        "\n",
        "[\n",
        "\\mathbf{x} = [3, 4, 5]\n",
        "]\n",
        "\n",
        "* Step 1: Calculate the Euclidean Norm\n",
        "[\n",
        "|\\mathbf{x}| = \\sqrt{3^2 + 4^2 + 5^2} = \\sqrt{9 + 16 + 25} = \\sqrt{50} \\approx 7.07\n",
        "]\n",
        "\n",
        "* Step 2: Normalize the Vector\n",
        "Now, to get the unit vector ( \\mathbf{x}' ):\n",
        "\n",
        "[\n",
        "\\mathbf{x}' = \\frac{\\mathbf{x}}{|\\mathbf{x}|} = \\left[\\frac{3}{7.07}, \\frac{4}{7.07}, \\frac{5}{7.07}\\right] \\approx [0.425, 0.566, 0.707]\n",
        "]"
      ],
      "metadata": {
        "id": "IlNGj7m2lzpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
        "example to illustrate its application.\n",
        "\n",
        "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction while preserving as much variance in the data as possible. It transforms a dataset of potentially correlated variables into a smaller set of uncorrelated variables called principal components. These components are linear combinations of the original variables and are ordered so that the first few retain most of the variation present in the original dataset.\n",
        "\n",
        "# Key Concepts of PCA\n",
        "1. Dimensionality Reduction: PCA is primarily used to reduce the number of features (dimensions) in a dataset, making it easier to visualize and process while retaining important information.\n",
        "\n",
        "2. Variance: The principal components are found by determining the directions (axes) in which the data varies the most. The first principal component has the highest variance, the second principal component has the second highest variance (and is orthogonal to the first), and so on.\n",
        "\n",
        "3. Orthogonality: The resulting principal components are uncorrelated (orthogonal) vectors, which helps in reducing redundancy in the data.\n",
        "\n",
        "4. Linear Transformation: PCA is a linear transformation technique; it assumes that the principal components can be expressed as linear combinations of the original features.\n",
        "\n",
        "# Steps Involved in PCA\n",
        "1. Standardization: The data is standardized to have a mean of zero and a standard deviation of one, especially if the features have different units or scales.\n",
        "\n",
        "2. Covariance Matrix Computation: Calculate the covariance matrix of the standardized data to understand how the features vary together.\n",
        "\n",
        "3. Eigenvalue and Eigenvector Calculation: Compute the eigenvalues and eigenvectors of the covariance matrix. Eigenvectors determine the direction of the new feature space, and eigenvalues indicate the magnitude (variance) along those directions.\n",
        "\n",
        "4. Selecting Principal Components: Select the top (k) eigenvectors that correspond to the largest (k) eigenvalues to form a new feature space. The number of principal components, (k), is much less than the original number of features.\n",
        "\n",
        "5. Transforming the Data: Project the original data onto the new feature space using the selected principal components."
      ],
      "metadata": {
        "id": "bjMWgi1pmjsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
        "Extraction? Provide an example to illustrate this concept.\n",
        "\n",
        "Principal Component Analysis (PCA) is a powerful technique that not only serves as a dimensionality reduction method but also plays a significant role in feature extraction. The relationship between PCA and feature extraction can be understood through the following points:\n",
        "\n",
        "# Relationship between PCA and Feature Extraction\n",
        "1. Dimensionality Reduction:\n",
        "\n",
        "* PCA reduces the number of features (dimensions) in a dataset while retaining as much information (variance) as possible. This reduction is beneficial for simplifying the dataset and mitigating the \"curse of dimensionality.\"\n",
        "2. New Feature Creation:\n",
        "\n",
        "* PCA creates new features (principal components) that are linear combinations of the original features. These new features may capture the underlying structure and patterns in the data better than the original features.\n",
        "3. Orthogonality:\n",
        "\n",
        "* The principal components generated by PCA are orthogonal (uncorrelated), which helps in removing redundancy and multicollinearity from the dataset.\n",
        "4. Data Representation:\n",
        "\n",
        "* The newly obtained principal components can often provide better performance in subsequent machine learning tasks compared to using the original features, as they can highlight important patterns while reducing noise.\n",
        "# Using PCA for Feature Extraction\n",
        "PCA can be used for feature extraction in the following way:\n",
        "\n",
        "1. Standardize the Data:\n",
        "\n",
        "* Center the data by subtracting the mean and scaling by the standard deviation.\n",
        "2. Compute Covariance Matrix:\n",
        "\n",
        "* Determine the covariance matrix to understand how the original features vary together.\n",
        "3. Find Eigenvalues and Eigenvectors:\n",
        "\n",
        "* Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues indicate the amount of variance captured by each principal component.\n",
        "4. Select Principal Components:\n",
        "\n",
        "* Choose a subset of the principal components (based on eigenvalues) to create a new feature space. The number of components selected depends on the explained variance desired.\n",
        "5. Transform Data:\n",
        "\n",
        "* Project the original data onto the new set of principal components, forming a new dataset with the selected features."
      ],
      "metadata": {
        "id": "Ogg858PEnE-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
        "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
        "preprocess the data.\n",
        "\n",
        "Min-Max scaling is a normalization technique used to rescale features to a specific range, typically from 0 to 1. It is particularly useful in scenarios where features have significantly different ranges and units, which is common in datasets with various attributes like price, rating, and delivery time.\n",
        "\n",
        "# Purpose of Min-Max Scaling\n",
        "1. Uniform Scale: Min-Max scaling transforms features to a uniform scale so that they can be compared on the same basis. This is especially important for algorithms sensitive to the scale of data, such as those based on distance metrics (e.g., k-nearest neighbors, clustering) or gradient-based optimization methods (e.g., neural networks).\n",
        "\n",
        "2. Preservation of Relationships: The scaling keeps the relationships and distributions among the features intact, which helps the model learn more effectively without distortion caused by differing scales.\n",
        "\n",
        "# Steps to Apply Min-Max Scaling to Preprocess the Data\n",
        "1. Identify Features: Identify the features in your dataset that require scaling. In this case, these may include:\n",
        "\n",
        "* Price: The cost of the food items.\n",
        "* Rating: Typically a score from 1 to 5 or similar.\n",
        "* Delivery Time: Time taken for delivery, which might be in minutes.\n",
        "\n",
        "2. Formula for Min-Max Scaling:\n",
        "The Min-Max scaling formula is given by:\n",
        "\n",
        "[\n",
        "X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (X) is the original value.\n",
        "* (X') is the scaled value.\n",
        "* (X_{min}) is the minimum value of the feature in the dataset.\n",
        "* (X_{max}) is the maximum value of the feature in the dataset.\n",
        "\n",
        "3. Calculate Minimum and Maximum: For each feature (price, rating, delivery time):\n",
        "\n",
        "* Calculate the minimum and maximum values from the training portion of your dataset.\n",
        "4. Apply Scaling:\n",
        "\n",
        "* Apply the Min-Max scaling transformation to each feature in the dataset according to the formula mentioned above.\n",
        "* For instance, if you have a price feature with a minimum of $5 and a maximum of $50, you would transform a price of $15 as follows:\n",
        "[\n",
        "\\text{Scaled Price} = \\frac{15 - 5}{50 - 5} = \\frac{10}{45} \\approx 0.222\n",
        "]\n",
        "\n",
        "5. Handling Test Data: When you apply the transformation to test data or new incoming data, it is critical to use the (X_{min}) and (X_{max}) calculated from the training data to ensure consistency.\n",
        "\n",
        "6. Final Scaled Dataset: After scaling, your dataset features will now lie in the range of 0 to 1. This transforms the original dataset of price, rating, and delivery time into a normalized format suitable for model training.\n",
        "\n",
        "* Example\n",
        "\n",
        "* Minimum Price = $10, Maximum Price = $50\n",
        "* Minimum Rating = 3.5, Maximum Rating = 5.0\n",
        "* Minimum Delivery Time = 25, Maximum Delivery Time = 60\n",
        "Applying Min-Max Scaling:\n",
        "\n",
        "1. Scaled Price:\n",
        "\n",
        "* For the first row: ((10 - 10) / (50 - 10) = 0)\n",
        "* For the second row: ((25 - 10) / (50 - 10) = 0.375)\n",
        "* For the third row: ((15 - 10) / (50 - 10) = 0.125)\n",
        "* For the fourth row: ((50 - 10) / (50 - 10) = 1)\n",
        "2. Scaled Rating:\n",
        "\n",
        "* For the first row: ((4.5 - 3.5) / (5.0 - 3.5) = 0.6667)\n",
        "* For the second row: ((4.0 - 3.5) / (5.0 - 3.5) = 0.3333)\n",
        "* For the third row: ((5.0 - 3.5) / (5.0 - 3.5) = 1.0)\n",
        "* For the fourth row: ((3.5 - 3.5) / (5.0 - 3.5) = 0.0)\n",
        "\n",
        "3. Scaled Delivery Time:\n",
        "\n",
        "* For the first row: ((30 - 25) / (60 - 25) = 0.1)\n",
        "* For the second row: ((40 - 25) / (60 - 25) = 0.5)\n",
        "* For the third row: ((25 - 25) / (60 - 25) = 0.0)\n",
        "* For the fourth row: ((60 - 25) / (60 - 25) = 1.0)"
      ],
      "metadata": {
        "id": "YoWuEqB-n3La"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
        "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
        "dimensionality of the dataset.\n",
        "\n",
        "Using Principal Component Analysis (PCA) for dimensionality reduction is an effective approach when working with complex datasets, such as those used for stock price prediction. Hereâ€™s a step-by-step explanation of how to use PCA to reduce the dimensionality of a dataset containing various features like company financial data and market trends:\n",
        "\n",
        "# Why Use PCA?\n",
        "1. High-Dimensional Dataset: Stock price prediction datasets can often contain many features (like revenue, earnings per share, market capitalization, industry indexes, etc.), leading to challenges such as overfitting and increased computational costs.\n",
        "\n",
        "2. Redundancy and Correlation: Many features may be correlated or redundant, which can distort the effectiveness of machine learning models. PCA helps uncover these correlations and consolidate variations into fewer dimensions.\n",
        "\n",
        "3. Improved Computational Efficiency: Reducing the number of dimensions can significantly speed up the training and evaluation of machine learning models.\n",
        "\n",
        "# Steps to Apply PCA for Dimensionality Reduction\n",
        "\n",
        "1. Collect Data: Gather the dataset, ensuring it contains relevant features such as:\n",
        "\n",
        "* Company financial metrics (e.g., revenue, net income, debt levels).\n",
        "* Technical indicators (e.g., moving averages, trading volume).\n",
        "* Market trends (e.g., overall market returns, sector performance).\n",
        "2. Handle Missing Values: Preprocess the data by imputing or removing missing values to ensure the dataset is clean and ready for analysis.\n",
        "\n",
        "3. Feature Selection: Identify the features you want to include in the PCA. Not every feature may be relevant, so you might need to consider domain knowledge or feature selection methods.\n",
        "\n",
        "# Step 2: Standardizing the Data\n",
        "PCA is sensitive to the scale of the data, so it is important to standardize the features:\n",
        "\n",
        "1. Standardize Each Feature: Center the dataset by subtracting the mean and scaling by the standard deviation for each feature. This can be done through the Z-score standardization technique.\n",
        "\n",
        "[\n",
        "X' = \\frac{X - \\mu}{\\sigma}\n",
        "]\n",
        "\n",
        "Where (X) is the original feature value, (\\mu) is the mean of the feature, and (\\sigma) is its standard deviation.\n",
        "\n",
        "# Step 3: Compute the Covariance Matrix\n",
        "1. Covariance Matrix: Calculate the covariance matrix of the standardized dataset to assess how pairs of features vary together. For a dataset (X) with (m) observations and (n) features, the covariance matrix will be an (n \\times n) matrix.\n",
        "\n",
        "[\n",
        "\\text{Cov}(X) = \\frac{1}{m-1} (X^T X)\n",
        "]\n",
        "\n",
        "# Step 4: Eigenvalues and Eigenvectors\n",
        "1. Compute Eigenvalues and Eigenvectors: Calculate the eigenvalues and eigenvectors of the covariance matrix. Each eigenvalue corresponds to a principal component indicating the variance explained by that component.\n",
        "\n",
        "2. Sort Eigenvalues: Rank the eigenvalues in descending order. The eigenvectors associated with these eigenvalues are the principal components, with the first principal component accounting for the largest variance.\n",
        "\n",
        "# Step 5: Select Principal Components\n",
        "1. Choose Number of Components: Decide how many principal components to keep. This can be based on a threshold of cumulative explained variance (e.g., keep enough components that explain 90-95% of the variance).\n",
        "\n",
        "[\n",
        "\\text{Cumulative Variance} = \\frac{\\sum \\text{selected eigenvalues}}{\\sum \\text{all eigenvalues}}\n",
        "]\n",
        "\n",
        "2. Form Feature Vector: Construct a matrix (feature vector) using the selected eigenvectors. If you choose (k) components, this will be an (n \\times k) matrix.\n",
        "\n",
        "# Step 6: Transform the Data\n",
        "1. Project Original Data: Transform the original standardized dataset into the new feature space by multiplying it with the feature vector (matrix of selected eigenvectors):\n",
        "\n",
        "[\n",
        "Z = X' W\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (Z) is the transformed dataset.\n",
        "* (X') is the standardized data.\n",
        "* (W) is the feature vector.\n",
        "\n",
        "# Step 7: Model Training and Evaluation\n",
        "1. Train Model: Use the reduced dataset (Z) with fewer dimensions as input features for your stock price prediction model (e.g., linear regression, decision trees, etc.).\n",
        "\n",
        "2. Evaluation: Assess the model's performance using metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or R-squared, and compare it with models trained on the original high-dimensional data to verify if PCA has enhanced performance."
      ],
      "metadata": {
        "id": "IdC4XOW6qOsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
        "values to a range of -1 to 1.\n",
        "\n",
        "To perform Min-Max scaling to transform the values in the dataset ([1, 5, 10, 15, 20]) to a range of ([-1, 1]), we will follow these steps:\n",
        "\n",
        "# Step 1: Understand the Min-Max Scaling Formula\n",
        "The standard formula for Min-Max scaling to a new range ([a, b]) is given by:\n",
        "\n",
        "[\n",
        "X' = a + \\frac{(X - X_{\\text{min}})}{(X_{\\text{max}} - X_{\\text{min}})} \\times (b - a)\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (X) is the original value.\n",
        "* (X') is the scaled value.\n",
        "* (X_{\\text{min}}) is the minimum value in the dataset.\n",
        "* (X_{\\text{max}}) is the maximum value in the dataset.\n",
        "* (a) and (b) are the new minimum and maximum values, respectively.\n",
        "For this question:\n",
        "\n",
        "* (a = -1)\n",
        "* (b = 1)\n",
        "# Step 2: Calculate the Minimum and Maximum Values\n",
        "From the dataset ([1, 5, 10, 15, 20]):\n",
        "\n",
        "* (X_{\\text{min}} = 1)\n",
        "* (X_{\\text{max}} = 20)\n",
        "# Step 3: Apply the Min-Max Scaling Formula\n",
        "Substituting the bounds and values into the formula:\n",
        "\n",
        "1. For (X = 1):\n",
        "[\n",
        "X' = -1 + \\frac{(1 - 1)}{(20 - 1)} \\times (1 - (-1)) = -1 + 0 \\times 2 = -1\n",
        "]\n",
        "\n",
        "2. For (X = 5):\n",
        "[\n",
        "X' = -1 + \\frac{(5 - 1)}{(20 - 1)} \\times (1 - (-1)) = -1 + \\frac{4}{19} \\times 2 = -1 + \\frac{8}{19} \\approx -1 + 0.4211 \\approx -0.5789\n",
        "]\n",
        "\n",
        "3. For (X = 10):\n",
        "[\n",
        "X' = -1 + \\frac{(10 - 1)}{(20 - 1)} \\times (1 - (-1)) = -1 + \\frac{9}{19} \\times 2 = -1 + \\frac{18}{19} \\approx -1 + 0.9474 \\approx -0.0526\n",
        "]\n",
        "\n",
        "4. For (X = 15):\n",
        "[\n",
        "X' = -1 + \\frac{(15 - 1)}{(20 - 1)} \\times (1 - (-1)) = -1 + \\frac{14}{19} \\times 2 = -1 + \\frac{28}{19} \\approx -1 + 1.4737 \\approx 0.4737\n",
        "]\n",
        "\n",
        "5. For (X = 20):\n",
        "[\n",
        "X' = -1 + \\frac{(20 - 1)}{(20 - 1)} \\times (1 - (-1)) = -1 + 1 \\times 2 = -1 + 2 = 1\n",
        "]\n",
        "\n",
        "# Final Scaled Values\n",
        "The transformed values after applying Min-Max scaling to the range ([-1, 1]) are:\n",
        "\n",
        "* For (1): (-1)\n",
        "* For (5): (-0.5789 \\approx -0.58)\n",
        "* For (10): (-0.0526 \\approx -0.05)\n",
        "* For (15): (0.4737 \\approx 0.47)\n",
        "* For (20): (1)"
      ],
      "metadata": {
        "id": "_HymVWE2rdsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
        "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
        "\n",
        "When performing feature extraction using Principal Component Analysis (PCA) for a dataset with features such as [height, weight, age, gender, blood pressure], several steps and considerations are critical in determining how many principal components to retain. Hereâ€™s a structured approach to this problem:\n",
        "\n",
        "# Step 1: Understand the Features\n",
        "1. Features in the Dataset:\n",
        "* Height: Continuous variable.\n",
        "* Weight: Continuous variable.\n",
        "* Age: Continuous variable.\n",
        "* Gender: Categorical variable (can be converted to numerical for PCA).\n",
        "* Blood Pressure: Continuous variable.\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "1. Convert Categorical Variable:\n",
        "\n",
        "* Since PCA requires numerical input, you'll need to convert the gender feature to a numerical format (e.g., 0 for male, 1 for female or use one-hot encoding).\n",
        "2. Standardization:\n",
        "\n",
        "* Scale the continuous features to have a mean of 0 and a standard deviation of 1, as PCA is sensitive to the scales of the features.\n",
        "# Step 3: Compute PCA\n",
        "1. Covariance Matrix:\n",
        "\n",
        "* Calculate the covariance matrix of the standardized dataset.\n",
        "2. Eigenvalues and Eigenvectors:\n",
        "\n",
        "* Obtain the eigenvalues and eigenvectors from the covariance matrix.\n",
        "3. Select Principal Components:\n",
        "\n",
        "* Sort the eigenvalues in descending order to determine the amount of variance explained by each principal component.\n",
        "\n",
        "# Step 4: Determine the Number of Principal Components to Retain\n",
        "1. Cumulative Explained Variance:\n",
        "* Calculate the cumulative explained variance by summing the sorted eigenvalues and dividing by the total variance (sum of all eigenvalues) to get a proportion.\n",
        "* Plot the cumulative explained variance against the number of principal components to visualize how much variance is retained.\n",
        "\n",
        "# Step 5: Choose the Number of Components\n",
        "1. Elbow Method:\n",
        "\n",
        "* Look for an \"elbow\" in the cumulative explained variance plot. This point indicates where adding more components yields diminishing returns in variance explained.\n",
        "2. Variance Explained Threshold:\n",
        "\n",
        "* Decide on a threshold percentage for the cumulative explained variance (commonly 90-95%). Choose the number of components that meets or exceeds this threshold.\n",
        "Example Consideration"
      ],
      "metadata": {
        "id": "-blv1gshsRPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PbMPPz2HtM5I"
      }
    }
  ]
}