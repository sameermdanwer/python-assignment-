{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPU7gEWf7Nh4YqRRsnEOHcF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameermdanwer/python-assignment-/blob/main/Neural_Network_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.  Create a folder in which you want to create the project, after that use the git init and the necessary\n",
        "commands to create the specific Git repositoryH\n",
        "\n",
        "To create a Git repository for your project, follow these steps:\n",
        "\n",
        "# 1. Create a Folder for Your Project\n",
        "First, create a folder where you want your project to reside. You can do this using the command line or terminal:\n",
        "\n",
        "On Windows:\n",
        "Open Command Prompt or PowerShell, and run the following command to create the folder:"
      ],
      "metadata": {
        "id": "TSba-fw8Goq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir my-project\n",
        "cd my-project\n"
      ],
      "metadata": {
        "id": "G6KkviwPHZ-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Initialize a Git Repository\n",
        "Once inside your project folder, you need to initialize a Git repository. Run the following command to do that"
      ],
      "metadata": {
        "id": "TZ9KL6-nHjT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git init\n"
      ],
      "metadata": {
        "id": "y0gunu8oHnQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Add Files to the Repository\n",
        "If you have files for your project already, you can add them to the repository. If not, you can create new files or a new project structure.\n",
        "\n",
        "For example, to create a simple file:"
      ],
      "metadata": {
        "id": "arpOL1HZHpJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "echo \"# My Project\" > README.md\n"
      ],
      "metadata": {
        "id": "Kuv3IJ0tHskw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git add .\n"
      ],
      "metadata": {
        "id": "RWdcLLXNHw-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Commit the Files\n",
        "Now commit the files to the Git repository"
      ],
      "metadata": {
        "id": "tQp8E8OQHt2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git commit -m \"Initial commit\"\n"
      ],
      "metadata": {
        "id": "V4kEtxWiH2Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. (Optional) Connect to a Remote Repository (GitHub, GitLab, etc.)\n",
        "If you want to link this local repository to a remote repository, you can do so by following these steps:\n",
        "\n",
        "Create a new repository on a platform like GitHub, GitLab, or Bitbucket.\n",
        "\n",
        "Once the repository is created, copy the repository URL (HTTPS or SSH).\n",
        "\n",
        "Add the remote repository to your local Git project with the following command:"
      ],
      "metadata": {
        "id": "TPZ_T6VAH463"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git remote add origin <remote_repository_url>\n"
      ],
      "metadata": {
        "id": "Jh2XFHIrH6mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git remote add origin https://github.com/username/my-project.git\n"
      ],
      "metadata": {
        "id": "qItk_BloH9yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Push to the Remote Repository\n",
        "To upload your local commits to the remote repository, run"
      ],
      "metadata": {
        "id": "QN9HjzqIIAV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git push -u origin master\n"
      ],
      "metadata": {
        "id": "GZmYGBWJIFQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git push -u origin main\n"
      ],
      "metadata": {
        "id": "dWelAaGiIHrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q 2. Create a separate environment so that you do not mess up with your base environmentH\n",
        "\n",
        "To avoid messing up your base environment when working on a project, it is a good practice to create a virtual environment. Virtual environments allow you to create isolated spaces for each project, where you can manage specific dependencies without affecting other projects or the base environment.\n",
        "\n",
        "Here’s how you can create and use a virtual environment:\n",
        "1. Install virtualenv (if needed)\n",
        "First, make sure you have the virtualenv tool installed. You can install it via pip if it's not already installed:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RyNet0u8IKm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install virtualenv\n"
      ],
      "metadata": {
        "id": "0Tunitp6Il3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, if you are using Python 3.3+, the built-in venv module can be used, so installing virtualenv is not necessary.\n",
        "\n",
        "# 2. Create a Virtual Environment\n",
        "Now, create a virtual environment in your project folder. Here's how to do that:\n",
        "\n",
        "On Windows:"
      ],
      "metadata": {
        "id": "azVRTSSQIo8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python -m venv myenv\n"
      ],
      "metadata": {
        "id": "9QZ7ZsFkIq2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, myenv is the name of the virtual environment folder. You can choose any name you like.\n",
        "\n",
        "# 3. Activate the Virtual Environment\n",
        "After creating the virtual environment, you need to activate it.\n",
        "\n",
        "On Windows:"
      ],
      "metadata": {
        "id": "4XEx3zIFIw04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myenv\\Scripts\\activate\n"
      ],
      "metadata": {
        "id": "To9oePmXI2RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once activated, your terminal will show the virtual environment’s name in the prompt, e.g., (myenv) at the beginning of the command line.\n",
        "\n",
        "# 4. Install Dependencies in the Virtual Environment\n",
        "Now that your virtual environment is activated, you can install any project-specific dependencies without affecting the base environment. For example:\n",
        "\n"
      ],
      "metadata": {
        "id": "jpUO5Jj3I6Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas matplotlib\n"
      ],
      "metadata": {
        "id": "7bZsSgtCI83n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Deactivate the Virtual Environment\n",
        "When you’re done working in the virtual environment, you can deactivate it by simply running:"
      ],
      "metadata": {
        "id": "nIPMcYppJIJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deactivate\n"
      ],
      "metadata": {
        "id": "_VlFTMOOJKoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Using requirements.txt for Dependency Management (Optional)\n",
        "If you want to ensure that others (or you on a different machine) can recreate the environment with the same dependencies, you can generate a requirements.txt file that lists all installed packages:"
      ],
      "metadata": {
        "id": "HkcB1eRBJMbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "id": "1Y_K9b-1JRbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.  Create the folder structure/directories and files using the python programme required for a ML project.\n",
        "You can refer the following project structureF\n",
        "< sr3\n",
        ", __init__.p*\n",
        ", logger.p*\n",
        ", exception.p*\n",
        ", utils.p*\n",
        ", component5\n",
        ", __init__.p*\n",
        ", data_ingestion.p*\n",
        ", data_transformation.p*\n",
        ", model_trainer.p*\n",
        ", pipelin4\n",
        ", __init__.p*\n",
        ", predict_pipeline.p*\n",
        ", train_pipeline.p*\n",
        "< import_data.p*\n",
        "< setup.p*\n",
        "< noteboo+\n",
        "< requirements.txt\n",
        "After this update the created folders and files to your git repository by pushing from your end and add\n",
        "following files from github and pull it to your source codeF\n",
        ", README.m!\n",
        ", LICENS\n",
        ", .gitignor4"
      ],
      "metadata": {
        "id": "A4_2WXz9JV2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a folder structure and necessary files for a Machine Learning (ML) project using a Python program, here's how you can proceed step-by-step.\n",
        "\n",
        "# Step 1: Define the Folder Structure\n",
        "We’ll create a project with the following structure:"
      ],
      "metadata": {
        "id": "39uqYMxxJvR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_project/\n",
        "│\n",
        "├── sr3/\n",
        "│   ├── __init__.py\n",
        "│   ├── logger.py\n",
        "│   ├── exception.py\n",
        "│   └── utils.py\n",
        "│\n",
        "├── component5/\n",
        "│   ├── __init__.py\n",
        "│   ├── data_ingestion.py\n",
        "│   ├── data_transformation.py\n",
        "│   └── model_trainer.py\n",
        "│\n",
        "├── pipeline4/\n",
        "│   ├── __init__.py\n",
        "│   ├── predict_pipeline.py\n",
        "│   └── train_pipeline.py\n",
        "│\n",
        "├── import_data.py\n",
        "├── setup.py\n",
        "├── notebook/\n",
        "├── requirements.txt\n",
        "├── README.md\n",
        "├── LICENSE\n",
        "└── .gitignore\n"
      ],
      "metadata": {
        "id": "PSXkLP-OJ1I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create Python Program to Generate this Structure\n",
        "Below is a Python program that will create this directory and file structure for you."
      ],
      "metadata": {
        "id": "xWUb9qh0J4T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def create_ml_project_structure(base_dir):\n",
        "    # Define the folder structure\n",
        "    folders = [\n",
        "        \"sr3\",\n",
        "        \"component5\",\n",
        "        \"pipeline4\",\n",
        "        \"notebook\"\n",
        "    ]\n",
        "\n",
        "    # Define the files to be created\n",
        "    files = {\n",
        "        \"sr3\": [\"__init__.py\", \"logger.py\", \"exception.py\", \"utils.py\"],\n",
        "        \"component5\": [\"__init__.py\", \"data_ingestion.py\", \"data_transformation.py\", \"model_trainer.py\"],\n",
        "        \"pipeline4\": [\"__init__.py\", \"predict_pipeline.py\", \"train_pipeline.py\"],\n",
        "        \"root\": [\"import_data.py\", \"setup.py\", \"requirements.txt\", \"README.md\", \"LICENSE\", \".gitignore\"]\n",
        "    }\n",
        "\n",
        "    # Create the directories and files\n",
        "    for folder in folders:\n",
        "        folder_path = os.path.join(base_dir, folder)\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "        # Create files inside each folder\n",
        "        for file in files.get(folder, []):\n",
        "            open(os.path.join(folder_path, file), 'w').close()\n",
        "\n",
        "    # Create root files\n",
        "    for file in files[\"root\"]:\n",
        "        open(os.path.join(base_dir, file), 'w').close()\n",
        "\n",
        "    print(f\"Project structure created under '{base_dir}'\")\n",
        "\n",
        "# Set the base directory for your project\n",
        "base_directory = 'ml_project'\n",
        "create_ml_project_structure(base_directory)\n"
      ],
      "metadata": {
        "id": "ci0FYFZwJ8Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Run the Python Program\n",
        "Run the Python program provided above, and it will create the following folder structure in the directory you specify (ml_project in this case). After the program runs, you should see:"
      ],
      "metadata": {
        "id": "KNfdsgJAJ_aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_project/\n",
        "│\n",
        "├── sr3/\n",
        "│   ├── __init__.py\n",
        "│   ├── logger.py\n",
        "│   ├── exception.py\n",
        "│   └── utils.py\n",
        "│\n",
        "├── component5/\n",
        "│   ├── __init__.py\n",
        "│   ├── data_ingestion.py\n",
        "│   ├── data_transformation.py\n",
        "│   └── model_trainer.py\n",
        "│\n",
        "├── pipeline4/\n",
        "│   ├── __init__.py\n",
        "│   ├── predict_pipeline.py\n",
        "│   └── train_pipeline.py\n",
        "│\n",
        "├── import_data.py\n",
        "├── setup.py\n",
        "├── notebook/\n",
        "├── requirements.txt\n",
        "├── README.md\n",
        "├── LICENSE\n",
        "└── .gitignore\n"
      ],
      "metadata": {
        "id": "EYD8yjOEKBfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Write the program for setup.py and the relevant dependencies in requirements.txt and generate\n",
        "egg.info folderH\n",
        "\n",
        "To create the setup.py and requirements.txt for your Machine Learning project and generate the necessary egg-info folder, follow the steps outlined below.\n",
        "\n",
        "# Step 1: Write the setup.py File\n",
        "The setup.py file is used for packaging and distributing your project. It contains metadata about your project, such as its name, version, author, and dependencies. Here's a basic example of a setup.py for your project"
      ],
      "metadata": {
        "id": "6JICDBjFKKCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from setuptools import setup, find_packages\n",
        "\n",
        "setup(\n",
        "    name=\"ml_project\",  # Name of your project\n",
        "    version=\"0.1\",  # Version number\n",
        "    packages=find_packages(),  # Automatically find and include all packages\n",
        "    install_requires=[  # List of dependencies\n",
        "        'numpy',\n",
        "        'pandas',\n",
        "        'scikit-learn',\n",
        "        'matplotlib',\n",
        "        'seaborn',\n",
        "        'requests',\n",
        "        'tensorflow',  # or 'torch', depending on your ML framework\n",
        "    ],\n",
        "    entry_points={\n",
        "        'console_scripts': [\n",
        "            # If you want to add command-line tools, list them here\n",
        "        ],\n",
        "    },\n",
        "    classifiers=[\n",
        "        'Development Status :: 3 - Alpha',\n",
        "        'Intended Audience :: Developers',\n",
        "        'License :: OSI Approved :: MIT License',\n",
        "        'Programming Language :: Python :: 3',\n",
        "        'Programming Language :: Python :: 3.8',  # Adjust to your Python version\n",
        "        'Programming Language :: Python :: 3.9',\n",
        "        'Programming Language :: Python :: 3.10',\n",
        "    ],\n",
        "    python_requires='>=3.6',  # Minimum Python version required\n",
        "    author=\"Your Name\",\n",
        "    author_email=\"your.email@example.com\",\n",
        "    description=\"A Machine Learning project to do XYZ\",\n",
        "    long_description=open('README.md').read(),  # Automatically load README.md as long description\n",
        "    long_description_content_type=\"text/markdown\",\n",
        "    url=\"https://github.com/yourusername/ml_project\",  # Your GitHub repo URL\n",
        ")\n"
      ],
      "metadata": {
        "id": "lELGEoO2Klfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create the requirements.txt File\n",
        "The requirements.txt file is used to list the dependencies that your project requires. These dependencies will be installed when someone installs your project using pip. The contents should include the packages listed in the install_requires section of setup.py."
      ],
      "metadata": {
        "id": "UfRZi669KrUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numpy\n",
        "pandas\n",
        "scikit-learn\n",
        "matplotlib\n",
        "seaborn\n",
        "requests\n",
        "tensorflow  # or pytorch, depending on which ML framework you're using\n"
      ],
      "metadata": {
        "id": "cxQo3FoTKuDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Generate the egg-info Folder\n",
        "To generate the egg-info folder, which contains metadata about your Python package, you need to create a distribution package. This is typically done using the setuptools tool.\n",
        "\n",
        "Run the following command in the root directory of your project where setup.py is located:"
      ],
      "metadata": {
        "id": "KN5352AkK40Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "python setup.py egg_info\n"
      ],
      "metadata": {
        "id": "CinsmDbAK9vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Verify the egg-info Folder: After running the command, you should see a folder like this in your project directory\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "senOcPs_K8xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_project.egg-info/\n",
        "├── dependency_links.txt\n",
        "├── PKG-INFO\n",
        "├── SOURCES.txt\n",
        "├── top_level.txt\n",
        "└── requires.txt\n"
      ],
      "metadata": {
        "id": "G323xOJDKyhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Additional Commands (Optional)\n",
        "If you want to create a distribution package (e.g., .tar.gz, .whl), you can run:"
      ],
      "metadata": {
        "id": "AtJJREzeLY95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python setup.py sdist bdist_wheel\n"
      ],
      "metadata": {
        "id": "9eBQdg7WLe-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: (Optional) Installing Your Package Locally\n",
        "To install your package locally for development or testing, you can use the following command:\n"
      ],
      "metadata": {
        "id": "xzThXPg1Lf8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -e .\n"
      ],
      "metadata": {
        "id": "3xuf07oDLm8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Directory Structure:"
      ],
      "metadata": {
        "id": "gG_jLcqbLpth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ml_project/\n",
        "│\n",
        "├── sr3/\n",
        "│   ├── __init__.py\n",
        "│   ├── logger.py\n",
        "│   ├── exception.py\n",
        "│   └── utils.py\n",
        "│\n",
        "├── component5/\n",
        "│   ├── __init__.py\n",
        "│   ├── data_ingestion.py\n",
        "│   ├── data_transformation.py\n",
        "│   └── model_trainer.py\n",
        "│\n",
        "├── pipeline4/\n",
        "│   ├── __init__.py\n",
        "│   ├── predict_pipeline.py\n",
        "│   └── train_pipeline.py\n",
        "│\n",
        "├── import_data.py\n",
        "├── setup.py\n",
        "├── notebook/\n",
        "├── requirements.txt\n",
        "├── README.md\n",
        "├── LICENSE\n",
        "├── .gitignore\n",
        "└── ml_project.egg-info/\n"
      ],
      "metadata": {
        "id": "rNe-oa9zLteg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Write the logging function in logger.py and exception function in exception.py file to be used for the\n",
        "project to track the progress when the ML project is run and to raise any exception when encountered.\n",
        "\n",
        "In an ML project, it's essential to track the progress of operations (such as data loading, model training, etc.) and raise exceptions when something goes wrong. Logging helps us keep track of the operations and errors, and exception handling helps in raising proper error messages when unexpected issues occur.\n",
        "\n",
        "Let's write the logger.py and exception.py files with the necessary functions to achieve these objectives.\n",
        "\n",
        "# 1. logger.py (Logging Function)\n",
        "The logger.py file will include a function to set up logging for your project. We'll configure it to log messages to a file and console, and you can customize it to log different levels (INFO, DEBUG, ERROR, etc.)."
      ],
      "metadata": {
        "id": "osBn2bNuL9B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "# Create a function to set up the logger\n",
        "def setup_logger(log_file=\"project.log\"):\n",
        "    # Create a logger\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.DEBUG)  # Set the global logging level\n",
        "\n",
        "    # Create a formatter for the log messages\n",
        "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # Create a console handler to log to the console\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(formatter)\n",
        "\n",
        "    # Create a file handler to log to a file (log_file is the default)\n",
        "    file_handler = logging.FileHandler(log_file)\n",
        "    file_handler.setFormatter(formatter)\n",
        "\n",
        "    # Add both handlers to the logger\n",
        "    logger.addHandler(console_handler)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# logger = setup_logger(\"ml_project.log\")\n",
        "# logger.info(\"This is an info message\")\n",
        "# logger.error(\"This is an error message\")\n"
      ],
      "metadata": {
        "id": "ltT1-3cxMmar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. exception.py (Exception Handling Functions)\n",
        "The exception.py file will define functions for raising and handling exceptions. We’ll define a custom exception class for project-specific errors and provide a helper function to log and raise exceptions"
      ],
      "metadata": {
        "id": "MqOF7_LwMnyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Custom Exception Class for ML Project\n",
        "class MLProjectException(Exception):\n",
        "    def __init__(self, message, errors=None):\n",
        "        super().__init__(message)\n",
        "        self.message = message\n",
        "        self.errors = errors\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"MLProjectException: {self.message} | {self.errors}\"\n",
        "\n",
        "\n",
        "# Function to handle exceptions in a way that they are logged\n",
        "def log_and_raise_exception(logger, message, errors=None):\n",
        "    \"\"\"\n",
        "    Logs the exception and raises it.\n",
        "    Arguments:\n",
        "    - logger: The logger instance to log the message.\n",
        "    - message: The error message to log.\n",
        "    - errors: Optional argument to include the error details.\n",
        "    \"\"\"\n",
        "    logger.error(message)\n",
        "    if errors:\n",
        "        logger.error(f\"Details: {errors}\")\n",
        "    raise MLProjectException(message, errors)\n",
        "\n",
        "# Example usage:\n",
        "# try:\n",
        "#     log_and_raise_exception(logger, \"Something went wrong in data processing\")\n",
        "# except MLProjectException as e:\n",
        "#     print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "IzLv98kzMrnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: How to Use These Modules in Your Project\n",
        "Using logger.py:\n",
        "\n",
        "First, import the setup_logger function to create a logger instance.\n",
        "Use the logger to log messages as the project progresses.\n",
        "Example in your project file:"
      ],
      "metadata": {
        "id": "nsqJYLXvMsrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sr3.logger import setup_logger\n",
        "\n",
        "# Set up the logger\n",
        "logger = setup_logger()\n",
        "\n",
        "# Log some information\n",
        "logger.info(\"Data ingestion started\")\n",
        "try:\n",
        "    # Your data ingestion code here\n",
        "    pass\n",
        "except Exception as e:\n",
        "    # Log an error if something goes wrong\n",
        "    logger.error(f\"Error occurred during data ingestion: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "oN2f7KIdMw9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Running Your ML Project with Logging and Exception Handling\n",
        "Once these files are implemented:\n",
        "\n",
        "Logger will track the progress of your project, such as when data is being ingested, transformed, or when a model is being trained.\n",
        "Exception Handling will ensure that if something goes wrong (e.g., a file is missing, a model fails to train, or data is corrupted), it’s logged and raised as an exception, which you can handle appropriately."
      ],
      "metadata": {
        "id": "zJbI7ODIM27A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Folder Structure (With Logger and Exception Files):"
      ],
      "metadata": {
        "id": "Sqn2KJ-4M7mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_project/\n",
        "│\n",
        "├── sr3/\n",
        "│   ├── __init__.py\n",
        "│   ├── logger.py        # Logging setup\n",
        "│   ├── exception.py     # Exception handling\n",
        "│   ├── utils.py\n",
        "│   └── other_modules.py\n",
        "│\n",
        "├── component5/\n",
        "│   ├── __init__.py\n",
        "│   ├── data_ingestion.py\n",
        "│   ├── data_transformation.py\n",
        "│   └── model_trainer.py\n",
        "│\n",
        "├── pipeline4/\n",
        "│   ├── __init__.py\n",
        "│   ├── predict_pipeline.py\n",
        "│   └── train_pipeline.py\n",
        "│\n",
        "├── import_data.py\n",
        "├── setup.py\n",
        "├── notebook/\n",
        "├── requirements.txt\n",
        "├── README.md\n",
        "├── LICENSE\n",
        "└── .gitignore\n"
      ],
      "metadata": {
        "id": "YdHVeq9YM4dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7.  In the notebook folder create a jupyter notebook inside it and do the following with the dataset\"\n",
        "7 Exploratory Data Analysi&\n",
        "7 Feature Engineerin:\n",
        "7 Model Trainin:\n",
        "7 Selection of best model using metri,"
      ],
      "metadata": {
        "id": "E2P96CNdM_e5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "To create a Jupyter notebook for your ML project inside the notebook folder and perform tasks such as Exploratory Data Analysis (EDA), Feature Engineering, Model Training, and Selection of the Best Model using metrics, follow the steps below:\n",
        "\n",
        "Step 1: Set Up Your Jupyter Notebook\n",
        "First, make sure that you have Jupyter installed in your environment. You can install Jupyter using the following command if it's not already installed:"
      ],
      "metadata": {
        "id": "XlLVwOJQNU5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a new notebook in the notebook folder. The structure will look like this:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dmLnyvlLNWaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_project/\n",
        "│\n",
        "├── notebook/\n",
        "│   ├── ml_project_analysis.ipynb  # Your Jupyter notebook\n",
        "│\n",
        "└── other directories and files\n"
      ],
      "metadata": {
        "id": "EiBJeR4UNcIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create the Notebook and Perform the Tasks\n",
        "Below is the Python code that should be inside the Jupyter notebook ml_project_analysis.ipynb. This code will walk through Exploratory Data Analysis (EDA), Feature Engineering, Model Training, and Model Selection using metrics.\n",
        "\n",
        "1. Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "1XV7jFxcNhKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.csv' with the actual path to your dataset)\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()\n",
        "\n",
        "# Basic statistics of the dataset\n",
        "df.describe()\n",
        "\n",
        "# Check for missing values\n",
        "df.isnull().sum()\n",
        "\n",
        "# Data types of each column\n",
        "df.dtypes\n",
        "\n",
        "# Visualizations\n",
        "# 1. Distribution of numerical features\n",
        "df.hist(bins=30, figsize=(20, 15))\n",
        "plt.show()\n",
        "\n",
        "# 2. Correlation matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# 3. Boxplot for identifying outliers\n",
        "sns.boxplot(data=df)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d0akYCFENilB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Feature Engineering"
      ],
      "metadata": {
        "id": "ygyao8vFNl35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering: Handling missing values\n",
        "# Example: Fill missing values with the median of the column\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# Example: Encoding categorical variables (One-Hot Encoding)\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Feature scaling (Standardization) for numerical features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "numerical_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# Split dataset into features (X) and target (y)\n",
        "X = df.drop('target_column', axis=1)  # Replace 'target_column' with your actual target column name\n",
        "y = df['target_column']\n"
      ],
      "metadata": {
        "id": "Ep6jJHcyNnrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Training"
      ],
      "metadata": {
        "id": "S7m_FGZXNo2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Import some common classification models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Instantiate the models\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "log_reg_model = LogisticRegression()\n",
        "svc_model = SVC()\n",
        "\n",
        "# Fit the models on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "svc_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "vYmE7FdPNrl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Selection of Best Model Using Metrics"
      ],
      "metadata": {
        "id": "vPn6axrgNv4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary metrics for model evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Predictions\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "log_reg_preds = log_reg_model.predict(X_test)\n",
        "svc_preds = svc_model.predict(X_test)\n",
        "\n",
        "# Evaluate the models using accuracy, precision, recall, F1 score, and ROC AUC\n",
        "models = ['Random Forest', 'Logistic Regression', 'SVM']\n",
        "predictions = [rf_preds, log_reg_preds, svc_preds]\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "roc_aucs = []\n",
        "\n",
        "for i, preds in enumerate(predictions):\n",
        "    accuracies.append(accuracy_score(y_test, preds))\n",
        "    precisions.append(precision_score(y_test, preds))\n",
        "    recalls.append(recall_score(y_test, preds))\n",
        "    f1_scores.append(f1_score(y_test, preds))\n",
        "    roc_aucs.append(roc_auc_score(y_test, preds))\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "model_comparison = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Accuracy': accuracies,\n",
        "    'Precision': precisions,\n",
        "    'Recall': recalls,\n",
        "    'F1 Score': f1_scores,\n",
        "    'ROC AUC': roc_aucs\n",
        "})\n",
        "\n",
        "model_comparison\n"
      ],
      "metadata": {
        "id": "w1vH-MOENxnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Folder Structure"
      ],
      "metadata": {
        "id": "i1kPNhFoN2WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_project/\n",
        "│\n",
        "├── notebook/\n",
        "│   ├── ml_project_analysis.ipynb  # Your Jupyter notebook with analysis\n",
        "│\n",
        "├── sr3/\n",
        "│   ├── __init__.py\n",
        "│   ├── logger.py\n",
        "│   ├── exception.py\n",
        "│   └── utils.py\n",
        "├── component5/\n",
        "├── pipeline4/\n",
        "├── requirements.txt\n",
        "├── setup.py\n",
        "├── README.md\n",
        "└── LICENSE\n"
      ],
      "metadata": {
        "id": "Fu7lmUdVN3mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Write a separate python program in import_data.py file to load the mentioned dataset from\n",
        "sklearn.load_dataset.load_breast_cancer to your MongoDBF\n",
        "\n",
        "\n",
        "To load the Breast Cancer dataset from sklearn.datasets.load_breast_cancer into MongoDB, we'll need to use a few tools:\n",
        "\n",
        "Scikit-learn: To load the dataset.\n",
        "Pandas: To convert the dataset into a DataFrame for easier manipulation.\n",
        "PyMongo: To interact with MongoDB and insert data.\n",
        "Steps to Create the import_data.py File:\n",
        "Install the necessary dependencies:\n",
        "\n",
        "You need to install pandas, scikit-learn, and pymongo if you haven't already."
      ],
      "metadata": {
        "id": "fSCq8gsKOLam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn pymongo\n"
      ],
      "metadata": {
        "id": "dJr2AtX4OPW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write the program to load the dataset and insert it into MongoDB."
      ],
      "metadata": {
        "id": "3IKgtrQ-OQ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Function to load the Breast Cancer dataset from sklearn\n",
        "def load_breast_cancer_data():\n",
        "    # Load the breast cancer dataset\n",
        "    data = load_breast_cancer()\n",
        "\n",
        "    # Convert the dataset into a DataFrame\n",
        "    df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "    df['target'] = data.target  # Add the target column\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to connect to MongoDB and insert data\n",
        "def insert_data_to_mongodb(df, db_name='ml_project_db', collection_name='breast_cancer_data'):\n",
        "    # Connect to MongoDB server (default localhost:27017)\n",
        "    client = MongoClient('mongodb://localhost:27017/')\n",
        "\n",
        "    # Select the database and collection\n",
        "    db = client[db_name]\n",
        "    collection = db[collection_name]\n",
        "\n",
        "    # Convert DataFrame to dictionary and insert into MongoDB\n",
        "    records = df.to_dict(orient='records')  # Convert DataFrame to a list of dictionaries\n",
        "    collection.insert_many(records)  # Insert multiple records at once\n",
        "\n",
        "    print(f\"Data inserted into MongoDB collection: {collection_name}\")\n",
        "\n",
        "# Main function\n",
        "if __name__ == '__main__':\n",
        "    # Load the dataset\n",
        "    breast_cancer_df = load_breast_cancer_data()\n",
        "\n",
        "    # Insert the dataset into MongoDB\n",
        "    insert_data_to_mongodb(breast_cancer_df)\n"
      ],
      "metadata": {
        "id": "8GRfN4iBOWla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. In data_ingestion.py write a program to load the same dataset from the MongoDB to your system in\n",
        "DataFrame formatF\n",
        "\n",
        "\n",
        "To load the Breast Cancer dataset from MongoDB to your system in DataFrame format, you can use PyMongo to fetch the data from the MongoDB collection and pandas to convert the data into a DataFrame.\n",
        "\n",
        "# Steps to Create the data_ingestion.py File:\n",
        "1. Install Required Libraries: If you haven't already installed the necessary libraries, use the following commands:"
      ],
      "metadata": {
        "id": "vPgqx1icOkRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymongo pandas\n"
      ],
      "metadata": {
        "id": "q_eAdY1zOqji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write the Program in data_ingestion.py: The script will connect to MongoDB, retrieve the dataset from the collection, and load it into a pandas DataFrame."
      ],
      "metadata": {
        "id": "qOsXNOWCOsDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Function to load data from MongoDB\n",
        "def load_data_from_mongodb(db_name='ml_project_db', collection_name='breast_cancer_data'):\n",
        "    # Connect to MongoDB server (default localhost:27017)\n",
        "    client = MongoClient('mongodb://localhost:27017/')\n",
        "\n",
        "    # Select the database and collection\n",
        "    db = client[db_name]\n",
        "    collection = db[collection_name]\n",
        "\n",
        "    # Fetch all records from the collection\n",
        "    records = collection.find()  # Returns a cursor\n",
        "\n",
        "    # Convert the cursor to a list of dictionaries and then to a pandas DataFrame\n",
        "    df = pd.DataFrame(list(records))\n",
        "\n",
        "    # Drop the MongoDB '_id' column if it exists\n",
        "    if '_id' in df.columns:\n",
        "        df.drop('_id', axis=1, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Main function\n",
        "if __name__ == '__main__':\n",
        "    # Load the dataset from MongoDB into a DataFrame\n",
        "    breast_cancer_df = load_data_from_mongodb()\n",
        "\n",
        "    # Display the first few rows of the DataFrame\n",
        "    print(breast_cancer_df.head())\n"
      ],
      "metadata": {
        "id": "UhZyT-bTOxua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Do the necessary feature engineering part in data_transformation.pyF\n",
        "\n",
        "Feature engineering is a crucial step in preparing the dataset for machine learning. It involves transforming raw data into features that improve the performance of machine learning models. In the case of the Breast Cancer dataset, typical feature engineering tasks may include:\n",
        "\n",
        "Handling missing values (though in this dataset, there are no missing values).\n",
        "Feature scaling (normalizing or standardizing numerical features).\n",
        "Encoding categorical features (though this dataset has only numerical features).\n",
        "Feature selection (choosing the most relevant features for the model).\n",
        "Adding new features (if needed).\n",
        "For the Breast Cancer dataset, the most relevant feature engineering steps would likely be scaling and potentially feature selection. In this case, we'll use StandardScaler to scale the numerical features so that they are on the same scale.\n",
        "\n",
        "# Steps to Implement in data_transformation.py:\n",
        "1. Standardize/Normalize features using StandardScaler from sklearn.\n",
        "2. Optional: Feature selection (if needed based on the problem, though this dataset is already well-structured)."
      ],
      "metadata": {
        "id": "Bf_uarduPAqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to perform feature engineering on the dataset\n",
        "def feature_engineering(df):\n",
        "    # Separate features (X) and target (y)\n",
        "    X = df.drop(columns=['target'])  # Drop the target column\n",
        "    y = df['target']  # Target column\n",
        "\n",
        "    # Feature Scaling: Standardize the features using StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Convert the scaled features back into a DataFrame\n",
        "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "    return X_scaled_df, y\n",
        "\n",
        "# Function to split the data into training and testing sets\n",
        "def split_data(X, y, test_size=0.2, random_state=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Main function to load, transform, and split the data\n",
        "if __name__ == '__main__':\n",
        "    # Load the data from MongoDB (assuming the `load_data_from_mongodb` function from previous steps)\n",
        "    # You can import the load function from data_ingestion.py if needed\n",
        "    from data_ingestion import load_data_from_mongodb\n",
        "\n",
        "    # Load the dataset\n",
        "    df = load_data_from_mongodb()\n",
        "\n",
        "    # Perform feature engineering (scaling)\n",
        "    X_scaled, y = feature_engineering(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = split_data(X_scaled, y)\n",
        "\n",
        "    # Print the results\n",
        "    print(\"Feature Engineering complete. Dataset is scaled and split into train/test sets.\")\n",
        "    print(f\"Training set size: {X_train.shape}\")\n",
        "    print(f\"Test set size: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "ehpK0t8ePQOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.  Create the Machine Learning model in model_trainer.pyF\n",
        "\n",
        "To create the machine learning model in the model_trainer.py file, we will follow these steps:\n",
        "\n",
        "Import necessary libraries:\n",
        "\n",
        "We'll use Logistic Regression as a baseline classification model since the Breast Cancer dataset is a binary classification problem. Alternatively, you can choose other classifiers like Random Forest, Support Vector Machines (SVM), etc.\n",
        "We'll also use accuracy_score and classification_report from scikit-learn to evaluate the model's performance.\n",
        "Train the model:\n",
        "\n",
        "The model will be trained using the training data (which is already preprocessed and scaled).\n",
        "Evaluate the model:\n",
        "\n",
        "We will evaluate the model's performance on the testing set using accuracy and a classification report.\n",
        "# Steps to Implement in model_trainer.py:\n",
        "* Train the model: We'll train the classifier using the training set (X_train, y_train).\n",
        "* Evaluate the model: We'll evaluate the trained model on the test set (X_test, y_test)."
      ],
      "metadata": {
        "id": "Jjx5ftwDPdWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib  # To save the trained model\n",
        "\n",
        "# Function to train the machine learning model\n",
        "def train_model(X_train, y_train):\n",
        "    # Create the Logistic Regression model\n",
        "    model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # Predict the target values for the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy score\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Generate a classification report\n",
        "    report = classification_report(y_test, y_pred)\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "# Function to save the trained model\n",
        "def save_model(model, filename='breast_cancer_model.pkl'):\n",
        "    # Save the model to disk\n",
        "    joblib.dump(model, filename)\n",
        "    print(f\"Model saved as {filename}\")\n",
        "\n",
        "# Main function\n",
        "if __name__ == '__main__':\n",
        "    # Import the pre-processed data (assuming feature engineering is done)\n",
        "    from data_transformation import split_data, feature_engineering\n",
        "    from data_ingestion import load_data_from_mongodb\n",
        "\n",
        "    # Load the dataset from MongoDB\n",
        "    df = load_data_from_mongodb()\n",
        "\n",
        "    # Perform feature engineering (scaling the features)\n",
        "    X_scaled, y = feature_engineering(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = split_data(X_scaled, y)\n",
        "\n",
        "    # Train the model\n",
        "    model = train_model(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy, report = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    # Print the evaluation results\n",
        "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "\n",
        "    # Save the trained model to disk\n",
        "    save_model(model)\n"
      ],
      "metadata": {
        "id": "T5RT0lSCPl5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "This script successfully trains a Logistic Regression model on the Breast Cancer dataset, evaluates its performance, and saves the trained model to a file. You can adapt the script to use other classification models and modify it for different datasets or machine learning tasks.\n",
        "\n",
        "Let me know if you need further help!"
      ],
      "metadata": {
        "id": "ecp3DjduPpnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.  Use Flask to deploy your project.\n",
        "\n",
        "To deploy your machine learning model using Flask, we'll create a simple Flask API that accepts input data (such as new patient features) and returns a prediction (whether the tumor is malignant or benign based on the trained model). Here’s how to deploy your ML project with Flask:\n",
        "\n",
        "Steps to Deploy Using Flask:"
      ],
      "metadata": {
        "id": "SIV9fkhuPush"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install Flask: If you haven't installed Flask, run the following command:"
      ],
      "metadata": {
        "id": "HWwM75gmQB-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Flask\n"
      ],
      "metadata": {
        "id": "US4B6mc-QETq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create a Flask App: The Flask app will provide an HTTP API that accepts JSON input and outputs predictions based on the machine learning model.\n",
        "\n",
        "3. Load the Trained Model: We'll use joblib to load the trained model saved in the previous step (breast_cancer_model.pkl).\n",
        "\n",
        "4. Create an Endpoint: We'll create an endpoint (/predict) that accepts a POST request with the features of a new sample, runs the prediction, and returns the result.\n",
        "\n",
        "5. Start the Flask Server: Once the app is ready, we'll run the Flask server and make the API available locally or on a server.\n",
        "\n",
        "# Step-by-Step Code for app.py:\n"
      ],
      "metadata": {
        "id": "9Ikkxu7XQFka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the trained model\n",
        "model = joblib.load('breast_cancer_model.pkl')  # Ensure your model file is in the same directory\n",
        "\n",
        "# Initialize the StandardScaler for feature scaling\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Endpoint for making predictions\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Get the JSON data from the request\n",
        "    data = request.get_json()\n",
        "\n",
        "    # Convert the data into a pandas DataFrame for processing\n",
        "    input_data = pd.DataFrame([data])\n",
        "\n",
        "    # Check if the input data has the correct number of features (30 features for Breast Cancer dataset)\n",
        "    if input_data.shape[1] != 30:\n",
        "        return jsonify({\"error\": \"Input data must have 30 features.\"}), 400\n",
        "\n",
        "    # Feature scaling: Scale the input data using the same scaler used during training\n",
        "    input_scaled = scaler.fit_transform(input_data)\n",
        "\n",
        "    # Predict using the loaded model\n",
        "    prediction = model.predict(input_scaled)\n",
        "\n",
        "    # Return the result as a JSON response\n",
        "    result = 'Malignant' if prediction[0] == 1 else 'Benign'\n",
        "\n",
        "    return jsonify({'prediction': result})\n",
        "\n",
        "# Run the app\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "6wdylNYVQXKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Input:"
      ],
      "metadata": {
        "id": "mSUSMO_sQauZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"mean radius\": 14.0,\n",
        "  \"mean texture\": 19.2,\n",
        "  \"mean perimeter\": 90.1,\n",
        "  \"mean area\": 654.0,\n",
        "  \"mean smoothness\": 0.1016,\n",
        "  \"mean compactness\": 0.1187,\n",
        "  \"mean concavity\": 0.1025,\n",
        "  \"mean concave points\": 0.0892,\n",
        "  \"mean symmetry\": 0.1785,\n",
        "  \"mean fractal dimension\": 0.05904,\n",
        "  \"se radius\": 0.2715,\n",
        "  \"se texture\": 0.3600,\n",
        "  \"se perimeter\": 1.517,\n",
        "  \"se area\": 22.03,\n",
        "  \"se smoothness\": 0.007283,\n",
        "  \"se compactness\": 0.01987,\n",
        "  \"se concavity\": 0.009658,\n",
        "  \"se concave points\": 0.01901,\n",
        "  \"se symmetry\": 0.02537,\n",
        "  \"se fractal dimension\": 0.003799,\n",
        "  \"worst radius\": 20.22,\n",
        "  \"worst texture\": 25.38,\n",
        "  \"worst perimeter\": 136.2,\n",
        "  \"worst area\": 1345.0,\n",
        "  \"worst smoothness\": 0.1413,\n",
        "  \"worst compactness\": 0.3138,\n",
        "  \"worst concavity\": 0.4268,\n",
        "  \"worst concave points\": 0.3470,\n",
        "  \"worst symmetry\": 0.4096,\n",
        "  \"worst fractal dimension\": 0.09744\n",
        "}\n"
      ],
      "metadata": {
        "id": "hvP6W5oAQcNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Using curl Command:"
      ],
      "metadata": {
        "id": "baoaSrP3QfYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "curl -X POST -H \"Content-Type: application/json\" -d '{\n",
        "  \"mean radius\": 14.0,\n",
        "  \"mean texture\": 19.2,\n",
        "  \"mean perimeter\": 90.1,\n",
        "  \"mean area\": 654.0,\n",
        "  \"mean smoothness\": 0.1016,\n",
        "  \"mean compactness\": 0.1187,\n",
        "  \"mean concavity\": 0.1025,\n",
        "  \"mean concave points\": 0.0892,\n",
        "  \"mean symmetry\": 0.1785,\n",
        "  \"mean fractal dimension\": 0.05904,\n",
        "  \"se radius\": 0.2715,\n",
        "  \"se texture\": 0.3600,\n",
        "  \"se perimeter\": 1.517,\n",
        "  \"se area\": 22.03,\n",
        "  \"se smoothness\": 0.007283,\n",
        "  \"se compactness\": 0.01987,\n",
        "  \"se concavity\": 0.009658,\n",
        "  \"se concave points\": 0.01901,\n",
        "  \"se symmetry\": 0.02537,\n",
        "  \"se fractal dimension\": 0.003799,\n",
        "  \"worst radius\": 20.22,\n",
        "  \"worst texture\": 25.38,\n",
        "  \"worst perimeter\": 136.2,\n",
        "  \"worst area\": 1345.0,\n",
        "  \"worst smoothness\": 0.1413,\n",
        "  \"worst compactness\": 0.3138,\n",
        "  \"worst concavity\": 0.4268,\n",
        "  \"worst concave points\": 0.3470,\n",
        "  \"worst symmetry\": 0.4096,\n",
        "  \"worst fractal dimension\": 0.09744\n",
        "}' http://localhost:5000/predict\n"
      ],
      "metadata": {
        "id": "mILOdJ1yQhJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "* You've now created a Flask-based web service to deploy your machine learning model.\n",
        "* This API can be used to make predictions by sending feature data in a POST request.\n",
        "* You can easily extend this setup to deploy your model to a cloud platform (e.g., AWS, Heroku, or Google Cloud)."
      ],
      "metadata": {
        "id": "uX7I98E2Qi5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yz0Zv0rgQqsq"
      }
    }
  ]
}