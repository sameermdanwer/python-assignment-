{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzumhKYb/kKKYIRUUA+6ZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameermdanwer/python-assignment-/blob/main/Support_Vector_Machines_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is the mathematical formula for a linear SVM?\n",
        "\n",
        "\n",
        "A linear Support Vector Machine (SVM) is used for binary classification tasks and aims to find the optimal hyperplane that separates two classes in the feature space. The mathematical formulation for a linear SVM involves the following components:\n",
        "\n",
        "1. **Linear Decision Function**: The decision function for a linear SVM can be expressed as:\n",
        "[\n",
        "f(x) = w^T x + b\n",
        "]\n",
        "where:\n",
        "\n",
        " * ( x ) is the input feature vector.\n",
        " * ( w ) is the weight vector (the coefficients that define the orientation of the hyperplane).\n",
        " * ( b ) is the bias term (which allows the hyperplane to be shifted).\n",
        "2. **Classification Rule** : The classification of a sample ( x ) is determined based on the sign of the decision function:\n",
        "[\n",
        "y = \\begin{cases}\n",
        "+1 & \\text{if } f(x) \\geq 0 \\\n",
        "-1 & \\text{if } f(x) < 0\n",
        "\\end{cases}\n",
        "]\n",
        "where ( y ) is the predicted class label.\n",
        "\n",
        "3. **Optimization Objective** : The goal of the SVM is to find the optimal ( w ) and ( b ) that maximize the margin between the two classes while also ensuring that the samples from each class are correctly classified. The optimization problem can be formulated as:\n",
        "[\n",
        "\\min_{w, b} \\quad \\frac{1}{2} | w |^2 \\quad \\text{subject to} \\quad y_i (w^T x_i + b) \\geq 1, \\quad \\forall i\n",
        "]\n",
        "for each sample ( (x_i, y_i) ) in the training set, where ( y_i \\in {+1, -1} ).\n",
        "\n",
        "This formulation maximizes the margin between the two classes while keeping the classification constraints satisfied. The optimization can be solved using methods such as Lagrange multipliers or quadratic programming."
      ],
      "metadata": {
        "id": "Q4FHZ2hMluoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the objective function of a linear SVM?\n",
        "\n",
        "The objective function of a linear Support Vector Machine (SVM) is defined to find a hyperplane that best separates the data points of two classes while maximizing the margin between them. The SVM aims to minimize a particular cost function while adhering to certain constraints.\n",
        "\n",
        "# Objective Function\n",
        "For a linear SVM, the objective function can be expressed as:\n",
        "\n",
        "[\n",
        "\\min_{w, b} \\quad \\frac{1}{2} | w |^2\n",
        "]\n",
        "\n",
        "**Components**:\n",
        "\n",
        "* ( | w |^2 ): This term represents the squared norm of the weight vector ( w ). Minimizing ( | w |^2 ) corresponds to maximizing the margin between the two classes. A smaller ( w ) leads to a larger margin.\n",
        "* Constraints:\n",
        "In conjunction with the objective function, the SVM must satisfy the constraints for all training samples ( (x_i, y_i) ):\n",
        "\n",
        "[\n",
        "y_i (w^T x_i + b) \\geq 1, \\quad \\forall i\n",
        "]\n",
        "\n",
        "where:\n",
        "\n",
        "  * ( x_i ) is the feature vector of the  \n",
        "  * ( i )-th training sample.\n",
        "( y_i \\in {+1, -1} ) is the corresponding class label.\n",
        "\n",
        "**Soft Margin**\n",
        "\n",
        "In the case of non-linearly separable data, a soft margin formulation is used. The soft-margin SVM introduces slack variables ( \\xi_i ) to allow for some misclassification, leading to the following objective function:\n",
        "\n",
        "[\n",
        "\\min_{w, b, \\xi} \\quad \\frac{1}{2} | w |^2 + C \\sum_{i=1}^N \\xi_i\n",
        "]\n",
        "\n",
        "# Explanation of Components:\n",
        "* ( C ): A positive regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error. A larger ( C ) means that more emphasis is put on correctly classifying all training examples (less margin) while a smaller ( C ) allows for a wider margin even at the risk of some misclassifications.\n",
        "* ( \\xi_i ): Slack variable that measures the degree of misclassification for each training sample."
      ],
      "metadata": {
        "id": "DHD2NcU_mchQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. What is the kernel trick in SVM?\n",
        "\n",
        "The kernel trick is a powerful technique used in Support Vector Machines (SVM) and other machine learning algorithms to enable the learning of nonlinear decision boundaries without explicitly transforming the input data into a higher-dimensional space. Here's a detailed explanation of what the kernel trick is and how it works:\n",
        "\n",
        "**Concept**\n",
        "\n",
        "1. **Higher-Dimensional Space**: Many datasets are not linearly separable in their original feature space. By projecting the data into a higher-dimensional space, it becomes possible to find a linear separator (hyperplane) that can effectively distinguish between classes.\n",
        "\n",
        "2. **Feature Transformation**: Traditionally, transforming the data from the original feature space ( \\mathcal{X} ) to a higher-dimensional feature space ( \\mathcal{Z} ) is done using a mapping function ( \\phi: \\mathcal{X} \\rightarrow \\mathcal{Z} ). However, explicitly calculating this transformation can be computationally expensive or infeasible, especially in very high-dimensional spaces.\n",
        "\n",
        "# The Kernel Trick\n",
        "Instead of explicitly calculating the coordinates of the data in the higher-dimensional space, the kernel trick uses a kernel function ( K(x_i, x_j) ) to compute the inner product in that transformed space efficiently. This allows SVM to operate in a high-dimensional feature space without the need to explicitly define or calculate the transformation ( \\phi ).\n",
        "\n",
        "Mathematically, if ( K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j) ) represents the inner product in the higher-dimensional space, then the kernel trick allows the SVM optimization problem to be expressed entirely in terms of ( K ).\n",
        "\n",
        "**Common Kernel Functions**\n",
        "\n",
        "1. **Linear Kernel**:\n",
        "[\n",
        "K(x_i, x_j) = x_i^T x_j\n",
        "]\n",
        "(Equivalent to no transformation.)\n",
        "\n",
        "2. **Polynomial Kernel**:\n",
        "[\n",
        "K(x_i, x_j) = (x_i^T x_j + c)^d\n",
        "]\n",
        "where ( c ) is a constant and ( d ) is the degree of the polynomial.\n",
        "\n",
        "3. **Radial Basis Function (RBF) or Gaussian Kernel**:\n",
        "[\n",
        "K(x_i, x_j) = \\exp\\left(-\\frac{|x_i - x_j|^2}{2\\sigma^2}\\right)\n",
        "]\n",
        "where ( \\sigma ) is a parameter that controls the width of the kernel.\n",
        "\n",
        "4. **Sigmoid Kerne**:\n",
        "[\n",
        "K(x_i, x_j) = \\tanh(\\alpha x_i^T x_j + c)\n",
        "]\n",
        "where ( \\alpha ) and ( c ) are kernel parameters."
      ],
      "metadata": {
        "id": "wc5j_iCKpRXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What is the role of support vectors in SVM Explain with example\n",
        "\n",
        "Support vectors play a crucial role in the functionality of Support Vector Machines (SVM). They are the data points that are closest to the decision boundary (hyperplane) and are critical in defining the position and orientation of that hyperplane. Here's an explanation of their role, along with an illustrative example.\n",
        "\n",
        "# Role of Support Vectors\n",
        "1. **Defining the Decision Boundary**: Support vectors are the elements of the training dataset that are used to define the optimal hyperplane. Removing any other data points that are not support vectors does not affect the position of the hyperplane, but removing support vectors would change it.\n",
        "\n",
        "2. **Margin Calculation**: The SVM algorithm aims to maximize the margin between the support vectors of opposite classes. The distance between the support vectors of both classes and the hyperplane defines the margin. A larger margin typically indicates better performance on unseen data.\n",
        "\n",
        "3. **Resilience to Noise**: SVMs are designed to be less sensitive to noise. Because the decision boundary is determined by support vectors, the overall model can be more robust against outliers since only a subset of data points influences the boundary.\n",
        "\n",
        "4. **Impact on Prediction**: Only support vectors are used in the prediction or decision-making process. Non-support vectors do not influence the outcome of the SVM classifier.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Let's consider a simple example with a two-dimensional dataset, where we want to classify points into two classes: Class A and Class B.\n",
        "\n",
        "Dataset:\n",
        "Imagine we have the following points:\n",
        "\n",
        "* Class A: (1, 2), (2, 3)\n",
        "* Class B: (3, 4), (4, 5), (5, 6)\n",
        "The points can be plotted on a 2D plane. Upon analyzing the plot, we observe that there is a clear linear boundary that separates Class A from Class B.\n",
        "\n",
        "Visual Representation\n",
        "The decision boundary (hyperplane) in a 2D space can look like this:"
      ],
      "metadata": {
        "id": "0sKtSJ1EqDd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "         |\n",
        "    B    |   B\n",
        "         |\n",
        "   ------------------\n",
        "         |\n",
        "         |   A\n",
        "     A   |"
      ],
      "metadata": {
        "id": "30Y-Q3blqwia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identification of Support Vectors\n",
        "1. **Support Vectors**: In this example, let's say the closest points to the decision boundary are:\n",
        "* Class A point: (2, 3)\n",
        "* Class B point: (3, 4)\n",
        "These two points are the support vectors because they are the nearest to the margin. They actually \"support\" the construction of the hyperplane.\n",
        "\n",
        "2. **Non-Support Vectors**: The other points, (1, 2) from Class A and (4, 5), (5, 6) from Class B, are not as close to the decision boundary. They do not determine the margin boundaries and thus do not affect the position of the hyperplane.\n"
      ],
      "metadata": {
        "id": "m53wQR-XqyR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
        "SVM?\n",
        "\n",
        "Support Vector Machines (SVMs) rely on the concepts of hyperplanes, margins, soft margins, and hard margins to classify data. Here, we'll illustrate these concepts through examples and associated graphs.\n",
        "\n",
        "# 1. Hyperplane\n",
        "A hyperplane in SVM is a flat affine subspace that separates different classes in the feature space. In two dimensions, a hyperplane is simply a line.\n",
        "\n",
        "**Example**:"
      ],
      "metadata": {
        "id": "4IULTz2nrG3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "          |         B\n",
        "   Class  |           B\n",
        "          |           B\n",
        "     A    |___________________\n",
        "          |         A\n",
        "          |"
      ],
      "metadata": {
        "id": "lq3duK7zrYda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this graph, the line (hyperplane) separates Class A from Class B. The goal of SVM is to position this hyperplane such that it maximizes the margin between the two classes.\n",
        "\n",
        "# 2. Margin\n",
        "The margin is the distance between the hyperplane and the nearest data points of each class (support vectors). SVM aims to maximize this margin.\n",
        "\n",
        "**Graph with Margins:**\n",
        "\n"
      ],
      "metadata": {
        "id": "lEARr3kRq4Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "          |         B\n",
        "   Class  |           B\n",
        "          |           B\n",
        "     A    |-----|-----|-----|----- (Hyperplane)\n",
        "          |    - |     |  -\n",
        "          |         A"
      ],
      "metadata": {
        "id": "wpNVMxQprixq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this graph, the dashed lines represent the margins. The distance between these lines and the hyperplane is the margin. The points where the margins touch the dashed lines are the support vectors.\n",
        "\n",
        "# 3. Hard Margin SVM\n",
        "Hard margin SVM is a type of SVM that seeks to find a hyperplane that perfectly separates the classes with no misclassifications. This is only possible when the data is linearly separable.\n",
        "\n",
        "**Examples**:"
      ],
      "metadata": {
        "id": "HCfmUzjlrkDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "          |         B\n",
        "   Class  |           B\n",
        "          |           B\n",
        "     A    |--------------| (Hyperplane)\n",
        "          |    A        |\n",
        "          |   A         |"
      ],
      "metadata": {
        "id": "qWwDJxkKrr7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, there are no points from Class A that are on the side of Class B and vice versa. The hyperplane perfectly separates Class A and Class B with maximal margin between the two classes.\n",
        "\n",
        "# 4. Soft Margin SVM\n",
        "Soft margin SVM allows for some misclassifications, which is useful for datasets that are not perfectly linearly separable. This approach introduces a margin of tolerance for misclassified points, thus allowing some data points to be on the wrong side of the margin.\n",
        "\n",
        "**Example**:"
      ],
      "metadata": {
        "id": "7pFzshA5rxXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "          |         B\n",
        "   Class  |           B\n",
        "          |         B   B\n",
        "     A    |----|-------|----- (Hyperplane)\n",
        "          |  A A      |   A\n",
        "          |  A        |"
      ],
      "metadata": {
        "id": "KKFBhLU_r3aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. SVM Implementation through Iris dataset.\n",
        "\n",
        "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
        "performance with the scikit-learn implementation.\n",
        "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
        "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
        "~ Compute the accuracy of the model on the testing setl\n",
        "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
        "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
        "the model.\n",
        "\n",
        "\n",
        "# Implementing Linear SVM Classifier from Scratch\n",
        "For this task, we will implement a linear SVM classifier from scratch using Python. Then we will compare its performance with the implementation from the scikit-learn library on the Iris dataset.\n",
        "\n",
        "**Steps to Complete the Task**\n",
        "\n",
        "Load the Iris Dataset: Use the scikit-learn library.\n",
        "Split the Dataset: Divide it into training and testing sets.\n",
        "Implement Linear SVM: Create a linear SVM classifier from scratch.\n",
        "Train the Classifier: Fit the SVM on the training dataset.\n",
        "Predict and Evaluate: Make predictions on the test set and calculate accuracy.\n",
        "Plot Decision Boundaries: Visualize the decision boundary.\n",
        "Experiment with Regularization Parameter (C): Vary (C) and assess its effect on performance.\n",
        "Implementation"
      ],
      "metadata": {
        "id": "LLJGGdVUr4zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # We'll use only the first two features for visualization\n",
        "y = iris.target\n",
        "\n",
        "# Convert target variable to binary for binary SVM implementation (Setosa vs. Not-Setosa)\n",
        "y_binary = (y == 0).astype(int)  # Class 0 vs Class 1 & 2\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Implementing Linear SVM from scratch\n",
        "class LinearSVM:\n",
        "    def __init__(self, learning_rate=1e-3, reg_strength=1e-3, num_iter=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.reg_strength = reg_strength\n",
        "        self.num_iter = num_iter\n",
        "        self.W = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        # Initialize weights\n",
        "        self.W = np.random.randn(num_features)\n",
        "\n",
        "        # Training process\n",
        "        for _ in range(self.num_iter):\n",
        "            hinge_losses = 1 - y * (X @ self.W)\n",
        "            hinge_losses[hinge_losses < 0] = 0  # Only positive losses\n",
        "            loss = (np.sum(hinge_losses) / num_samples) + (self.reg_strength * np.sum(self.W ** 2))\n",
        "\n",
        "            # Gradient calculation\n",
        "            indicator = hinge_losses > 0\n",
        "            dW = -((X.T @ (indicator.astype(int) * y)) / num_samples) + (2 * self.reg_strength * self.W))\n",
        "            self.W -= self.lr * dW  # Update weights\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.where(X @ self.W >= 0, 1, -1)  # Binary classification\n",
        "\n",
        "# Train the Linear SVM Classifier\n",
        "svm = LinearSVM(learning_rate=1e-3, reg_strength=0.1, num_iter=1000)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of custom Linear SVM: {accuracy:.4f}\")\n",
        "\n",
        "# Scikit-learn SVM model for comparison\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train scikit-learn SVM\n",
        "sklearn_svm = SVC(kernel='linear', C=0.1)\n",
        "sklearn_svm.fit(X_train, y_train)\n",
        "y_pred_sklearn = sklearn_svm.predict(X_test)\n",
        "\n",
        "# Compare accuracy\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"Accuracy of scikit-learn Linear SVM: {accuracy_sklearn:.4f}\")\n",
        "\n",
        "# Decision boundary plotting function\n",
        "def plot_decision_boundary(X, y, model):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
        "    plt.xlabel(\"Sepal Length\")\n",
        "    plt.ylabel(\"Sepal Width\")\n",
        "    plt.title(\"SVM Decision Boundary (Custom Implementation)\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot decision boundary for the custom SVM\n",
        "plot_decision_boundary(X_test, y_test, svm)\n",
        "\n",
        "# Plot decision boundary for scikit-learn SVM\n",
        "plot_decision_boundary(X_test, y_test, sklearn_svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "WJAMHQHDsqf5",
        "outputId": "b5e1809b-92dc-4124-c2a5-ede749eb589f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unmatched ')' (<ipython-input-1-24c4b9b31300>, line 45)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-24c4b9b31300>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    dW = -((X.T @ (indicator.astype(int) * y)) / num_samples) + (2 * self.reg_strength * self.W))\u001b[0m\n\u001b[0m                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with Regularization Parameter (C)\n",
        "To experiment with different values of the regularization parameter (C) in the scikit-learn SVM, you can modify the C parameter in the instantiation of SVC and rerun the training process and evaluation. Then observe how the accuracy and decision boundary change.\n",
        "\n",
        "You can adjust the C value in the code, like so:"
      ],
      "metadata": {
        "id": "FUf5MOmts0Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sklearn_svm = SVC(kernel='linear', C=your_value_here)"
      ],
      "metadata": {
        "id": "zioKcK-is276"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AijG0XkEs5AA"
      }
    }
  ]
}