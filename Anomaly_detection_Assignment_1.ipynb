{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkUPMJguXpOPClVu0ouM34",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameermdanwer/python-assignment-/blob/main/Anomaly_detection_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is anomaly detection and what is its purpose?\n",
        "\n",
        "\n",
        "Anomaly detection is a process used in data analysis and machine learning to identify unusual patterns, data points, or events that deviate from a dataset's expected behavior. These \"anomalies\" or \"outliers\" can indicate significant or rare occurrences that require attention. The primary purpose of anomaly detection is to identify instances that differ significantly from the norm, which can help in various applications, such as:\n",
        "\n",
        "1. **Fraud Detection**: Identifying unusual transactions or activities in finance, such as credit card fraud or insider trading.\n",
        "2. **Network Security**: Detecting abnormal behavior in network traffic, which could indicate cybersecurity threats or attacks.\n",
        "3. **Quality Control**: Spotting defective products in manufacturing processes to maintain high standards and reduce wastage.\n",
        "4. **Predictive Maintenance**: Recognizing irregular patterns in machinery data, enabling early intervention to prevent equipment failure.\n",
        "5.**Health Monitoring**: Identifying unusual patterns in medical data, such as abnormal heart rate or unusual symptoms, to detect potential health issues early.\n",
        "\n",
        "Anomaly detection techniques can range from statistical methods (like Z-scores or hypothesis testing) to machine learning approaches (like clustering, isolation forests, and deep learning-based models). Its purpose is to help organizations and individuals make informed, proactive decisions based on data that might otherwise go unnoticed."
      ],
      "metadata": {
        "id": "a7iejdGwRcRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What are the key challenges in anomaly detection?\n",
        "\n",
        "\n",
        "Anomaly detection can be challenging due to the complex nature of real-world data and the varied contexts in which anomalies appear. Here are some key challenges commonly faced in anomaly detection:\n",
        "\n",
        "1. **Defining \"Anomaly\"**: Anomalies can vary significantly across different domains, making it hard to establish a universal definition. In some cases, an anomaly is simply a rare data point, while in others, it might be an unexpected trend or a subtle deviation. Defining what constitutes an anomaly in a given context is a foundational challenge.\n",
        "\n",
        "2. **Data Imbalance**: In most datasets, anomalies are rare compared to normal data, leading to highly imbalanced datasets. This makes it difficult for models to learn to detect anomalies, as they have far fewer examples of anomalous behavior to learn from.\n",
        "\n",
        "3. **Lack of Labeled Data**: Many datasets lack labeled anomalies, especially in unsupervised settings. Labeling anomalies can be expensive, time-consuming, and subjective, which makes training supervised models for anomaly detection difficult.\n",
        "\n",
        "4. **High Variability and Complex Patterns**: Anomalies can take many forms and may not follow consistent patterns. For instance, in cybersecurity, anomalies might change rapidly as attackers adjust tactics. This high variability makes it challenging for traditional models to adapt.\n",
        "\n",
        "5. **Evolving Data and Concept Drift**: In many domains, what constitutes \"normal\" behavior changes over time (known as concept drift). Models need to adapt to these changes or risk incorrectly classifying new, normal patterns as anomalies.\n",
        "\n",
        "6. **High Dimensionality**: In datasets with many features, it becomes difficult to detect anomalies due to the \"curse of dimensionality.\" High-dimensional data can make it hard for models to identify relevant features, leading to overfitting or underfitting.\n",
        "\n",
        "7. **Real-Time Detection Requirements**: In applications like fraud detection and network security, anomalies need to be detected in real-time. This requires fast, efficient models that can handle large volumes of data and respond quickly to potential threats.\n",
        "\n",
        "8. **False Positives and False Negatives**: Striking the right balance between detecting true anomalies and avoiding false alarms is difficult. High false positive rates can lead to alert fatigue, where too many false alarms reduce trust in the system. On the other hand, false negatives can result in missed detections of critical anomalies.\n",
        "\n",
        "9. **Scalability**: Anomaly detection in large-scale datasets, such as those generated by IoT devices or high-traffic web applications, requires models that can scale efficiently without compromising accuracy.\n",
        "\n",
        "Addressing these challenges requires selecting appropriate methods and models, designing robust systems, and continuously refining approaches as new data and requirements emerge.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "it8FT_vTSKLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
        "\n",
        "\n",
        "Unsupervised and supervised anomaly detection are two different approaches to identifying anomalies in data, and they differ primarily in terms of labeled data availability and their respective methods. Here’s a breakdown of each:\n",
        "\n",
        "# **1. Supervised Anomaly Detection**\n",
        "* **Definition**: Supervised anomaly detection uses labeled data, where both normal and anomalous instances are labeled, to train a model. This labeled data enables the model to learn specific patterns associated with both normal and abnormal behaviors.\n",
        "* **Methods**: Traditional machine learning methods, like classification algorithms (e.g., Support Vector Machines, Decision Trees, Neural Networks), are often used in supervised anomaly detection. The model learns to distinguish between normal and anomalous examples based on their labels.\n",
        "* **Advantages**: Supervised anomaly detection tends to be more accurate when labeled data is available, as the model learns directly from examples of what constitutes an anomaly.\n",
        "* **Disadvantages**: Labeled anomaly data is often scarce and expensive to obtain, especially in domains where anomalies are rare. This approach is also less flexible when new types of anomalies arise, as the model must be retrained with new labeled examples.\n",
        "# **2. Unsupervised Anomaly Detection**\n",
        "* **Definition**: Unsupervised anomaly detection does not require labeled data. Instead, it assumes that anomalies are rare and exhibit patterns significantly different from the majority of the data. This approach is often used when labeled data is unavailable or impractical to obtain.\n",
        "* **Methods**: Techniques in unsupervised anomaly detection include clustering (e.g., DBSCAN, k-means), statistical methods (e.g., Gaussian distribution, Z-scores), and distance-based methods (e.g., Isolation Forest, One-Class SVM). The model detects outliers by identifying data points that do not conform to the dominant pattern of the dataset.\n",
        "* **Advantages**: Unsupervised methods are more flexible, can handle datasets without labeled examples, and can detect novel anomalies. They’re especially useful when anomaly characteristics are not well-defined.\n",
        "* **Disadvantages**: Unsupervised anomaly detection can be less accurate, as it relies on assumptions about data distribution and frequency. It can also produce more false positives, since normal variations may sometimes be mistakenly flagged as anomalies."
      ],
      "metadata": {
        "id": "DcJi5HPJTILM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What are the main categories of anomaly detection algorithms?\n",
        "\n",
        "Anomaly detection algorithms can be broadly categorized based on the techniques they use to identify outliers and anomalies in data. Here are the main categories of anomaly detection algorithms:\n",
        "\n",
        "**1. Statistical Methods**\n",
        "\n",
        "* Description: These methods rely on statistical properties of the data, assuming that normal data points follow a known probability distribution. Data points that deviate significantly from this distribution are considered anomalies.\n",
        "* Examples:\n",
        " * Z-score: Identifies anomalies by calculating the standard deviation of each point from the mean.\n",
        " * Gaussian Mixture Model (GMM): Assumes data is a mix of Gaussian distributions and identifies points that don’t fit well within these distributions.\n",
        "* Use Case: Effective when the data follows a known distribution (e.g., normal distribution) and is well-structured.\n",
        "**2. Distance-Based Methods**\n",
        "\n",
        "* Description: These methods assume that normal data points are clustered closely together, and outliers are far from these clusters. They calculate distances between points and consider those far from their neighbors as anomalies.\n",
        "* Examples:\n",
        " * k-Nearest Neighbors (k-NN): Detects anomalies based on the distance of each data point to its k-nearest neighbors.\n",
        " * Distance-Based Outlier Detection (DBOD): Measures the distance to a point’s nearest neighbors and flags points with greater-than-expected distances as anomalies.\n",
        "* Use Case: Suitable for lower-dimensional data and situations where data points form distinct clusters.\n",
        "**3. Density-Based Methods**\n",
        "\n",
        "* Description: These methods examine the density of data points in various regions, assuming that normal data points belong to dense regions, while anomalies exist in sparse regions.\n",
        "* Examples:\n",
        " * Local Outlier Factor (LOF): Measures the local density deviation of data points relative to their neighbors, identifying anomalies as points with a significantly lower density.\n",
        " *  DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Clusters data points based on density, with unclustered points classified as anomalies.\n",
        "* Use Case: Effective in datasets where normal data points form dense clusters and anomalies are in sparse areas.\n",
        "**4. Clustering-Based Methods**\n",
        "\n",
        "* Description: These methods use clustering algorithms to group data points, assuming that anomalies do not belong to any cluster or belong to small, separate clusters.\n",
        "* Examples:\n",
        " * k-means: After clustering data into k clusters, points that are far from cluster centroids can be flagged as anomalies.\n",
        " * Hierarchical Clustering: Anomalies can be identified as points that don’t belong to any prominent cluster in a hierarchical tree.\n",
        "* Use Case: Useful when data has a natural clustering structure and anomalies can be identified based on their distance from clusters.\n",
        "**5. Isolation-Based Methods**\n",
        "\n",
        "* Description: These methods isolate anomalies by randomly partitioning data points and measuring the depth of each data point in a tree structure. Anomalies are easier to isolate than normal points, resulting in shorter average path lengths.\n",
        "* Examples:\n",
        " * Isolation Forest: Constructs a forest of random trees and calculates the path length of each data point. Shorter paths indicate anomalies.\n",
        "* Use Case: Suitable for high-dimensional datasets and situations where anomalies are isolated from the majority of data.\n",
        "**6. Domain-Specific Models**\n",
        "\n",
        "* Description: These models use domain-specific knowledge and customized rules to detect anomalies. They can include combinations of statistical thresholds, rule-based logic, or expert-defined heuristics tailored to a particular field.\n",
        "* Examples: Anomaly detection in manufacturing (based on tolerance levels), medical diagnostics (based on physiological norms), and financial fraud detection (based on transaction patterns).\n",
        " *  Use Case: Highly effective in cases where domain knowledge is available and helps define what constitutes an anomaly.\n",
        "**7. Machine Learning-Based Methods**\n",
        "\n",
        "Supervised: Require labeled data to classify instances as normal or anomalous.\n",
        "* Examples: Support Vector Machines (SVM), Neural Networks, Decision Trees.\n",
        " * Unsupervised: Detect anomalies in unlabeled data, identifying points that deviate from learned patterns.\n",
        " * Examples: Autoencoders, One-Class SVM, Self-Organizing Maps (SOM).\n",
        " * Semi-Supervised: Trained primarily on normal data and detects deviations when applied to new, potentially anomalous data.\n",
        "* Example: Deep learning models like autoencoders that learn to reconstruct normal patterns and flag high-reconstruction-error instances as anomalies.\n",
        "**8. Deep Learning-Based Methods**\n",
        "\n",
        "* Description: These methods leverage deep neural networks to capture complex patterns in high-dimensional data, often using reconstruction-based approaches.\n",
        "* Examples:\n",
        " * Autoencoders: Train on normal data to reconstruct input, and classify instances with high reconstruction error as anomalies.\n",
        " * Recurrent Neural Networks (RNNs): Used in time series anomaly detection to identify abnormal patterns over time.\n",
        "* Use Case: Suitable for high-dimensional data, complex temporal patterns, and applications where traditional methods struggle to capture intricate structures (e.g., image, audio, and sequential data).\n",
        "\n",
        "Each category has its strengths and is suited to specific types of data, so the choice of algorithm often depends on the dataset structure, availability of labeled data, and the characteristics of anomalies expected."
      ],
      "metadata": {
        "id": "UIud7YBhT3Iz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
        "\n",
        "\n",
        "Distance-based anomaly detection methods rely on several key assumptions about the data and how anomalies differ from normal instances. These assumptions are critical for the methods to work effectively. Here are the main assumptions made by distance-based anomaly detection methods:\n",
        "\n",
        "1. **Normal Data Points Are Closer Together**: Distance-based methods assume that normal data points tend to cluster together in a feature space, meaning that they are relatively close to each other based on some distance metric (e.g., Euclidean, Manhattan).\n",
        "\n",
        "2. **Anomalies Are Distant from Normal Data Points**: These methods assume that anomalies are located far from the dense clusters of normal data points. Anomalies are expected to lie at a greater distance from their nearest neighbors or have a higher average distance to other points compared to normal instances.\n",
        "\n",
        "3. **Distance Metric Reflects Data Characteristics**: The methods assume that the chosen distance metric accurately captures the differences in the data. This assumption is crucial, as using an inappropriate distance metric can lead to poor anomaly detection performance. For example, Euclidean distance might work well for low-dimensional data but struggle in high-dimensional spaces.\n",
        "\n",
        "4. **Data Has a Meaningful Structure**: Distance-based methods often assume that the data has some inherent structure, such as clusters or dense regions. They rely on the idea that normal data forms coherent groups or patterns, making isolated points (i.e., anomalies) easy to detect.\n",
        "\n",
        "5. **Uniformity in Feature Scales**: Distance-based methods assume that features are on similar scales, as differences in scale can disproportionately impact the calculated distances. If one feature has a larger range than others, it may dominate the distance calculations, leading to biased results.\n",
        "\n",
        "6. **Anomalies Are Rare**: These methods typically assume that anomalies are rare compared to normal instances. This aligns with the expectation that only a few points will fall outside the normal data clusters, which are more densely populated.\n",
        "\n",
        "These assumptions may not always hold, especially in high-dimensional data or datasets with complex patterns, so it’s essential to evaluate these assumptions and apply preprocessing steps like normalization or dimensionality reduction to improve the effectiveness of distance-based anomaly detection."
      ],
      "metadata": {
        "id": "phaKWIIdVheb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. How does the LOF algorithm compute anomaly scores?\n",
        "\n",
        "\n",
        "The Local Outlier Factor (LOF) algorithm is a popular density-based method for anomaly detection. It assigns an anomaly score to each data point based on how isolated the point is compared to its neighbors, with higher scores indicating greater likelihood of being an anomaly. The LOF algorithm is based on the concept of local density and calculates the anomaly score using the following steps:\n",
        "\n",
        "# **Steps to Compute LOF Scores**\n",
        "1. **Determine the k-Nearest Neighbors (k-NN)**\n",
        "\n",
        "* For each data point\n",
        "𝑝\n",
        "p, find its\n",
        "𝑘\n",
        "k-nearest neighbors based on a chosen distance metric (usually Euclidean distance). The parameter\n",
        "𝑘\n",
        "k controls the neighborhood size and affects how \"local\" the density estimation is.\n",
        "2. **Calculate the Reachability Distance**:\n",
        "\n",
        "* For each pair of points\n",
        "𝑝\n",
        "p and\n",
        "𝑜\n",
        "o (where\n",
        "𝑜\n",
        "o is one of\n",
        "𝑝\n",
        "p’s\n",
        "𝑘\n",
        "k-nearest neighbors), compute the reachability distance between them. The reachability distance is defined as:\n",
        "reachability_dist\n",
        "𝑘\n",
        "(\n",
        "𝑝\n",
        ",\n",
        "𝑜\n",
        ")\n",
        "=\n",
        "max\n",
        "⁡\n",
        "(\n",
        "distance\n",
        "(\n",
        "𝑝\n",
        ",\n",
        "𝑜\n",
        ")\n",
        ",\n",
        "k-distance\n",
        "(\n",
        "𝑜\n",
        ")\n",
        ")\n",
        "reachability_dist\n",
        "k\n",
        "​\n",
        " (p,o)=max(distance(p,o),k-distance(o))\n",
        "* Here,\n",
        "k-distance\n",
        "(\n",
        "𝑜\n",
        ")\n",
        "k-distance(o) is the distance from\n",
        "𝑜\n",
        "o to its\n",
        "𝑘\n",
        "k-nearest neighbor. The reachability distance ensures that points close to dense clusters have higher reachability distances when compared to those in sparse areas, which helps to prevent noise from affecting density calculations.\n",
        "3. **Calculate the Local Reachability Density (LRD)**:\n",
        "\n",
        "* The local reachability density of a point\n",
        "𝑝\n",
        "p is the inverse of the average reachability distance between\n",
        "𝑝\n",
        "p and its\n",
        "𝑘\n",
        "k-nearest neighbors. Mathematically, it is given by:\n",
        "LRD\n",
        "𝑘\n",
        "(\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝑘\n",
        "∑\n",
        "𝑜\n",
        "∈\n",
        "k-NN\n",
        "(\n",
        "𝑝\n",
        ")\n",
        "reachability_dist\n",
        "𝑘\n",
        "(\n",
        "𝑝\n",
        ",\n",
        "𝑜\n",
        ")\n",
        "LRD\n",
        "k\n",
        "​\n",
        " (p)=\n",
        "∑\n",
        "o∈k-NN(p)\n",
        "​\n",
        " reachability_dist\n",
        "k\n",
        "​\n",
        " (p,o)\n",
        "k\n",
        "​\n",
        "\n",
        "* This density metric captures the average \"closeness\" of point\n",
        "𝑝\n",
        "p to its neighbors. A lower LRD indicates that\n",
        "𝑝\n",
        "p is in a sparser region, potentially making it more likely to be an anomaly.\n",
        "4. **Compute the Local Outlier Factor (LOF) Score**:\n",
        "\n",
        "* The LOF score for point\n",
        "𝑝\n",
        "p is calculated by comparing its local reachability density with the local reachability densities of its\n",
        "𝑘\n",
        "k-nearest neighbors:\n",
        "LOF\n",
        "𝑘\n",
        "(\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "∑\n",
        "𝑜\n",
        "∈\n",
        "k-NN\n",
        "(\n",
        "𝑝\n",
        ")\n",
        "LRD\n",
        "𝑘\n",
        "(\n",
        "𝑜\n",
        ")\n",
        "LRD\n",
        "𝑘\n",
        "(\n",
        "𝑝\n",
        ")\n",
        "𝑘\n",
        "LOF\n",
        "k\n",
        "​\n",
        " (p)=\n",
        "k\n",
        "∑\n",
        "o∈k-NN(p)\n",
        "​\n",
        "  \n",
        "LRD\n",
        "k\n",
        "​\n",
        " (p)\n",
        "LRD\n",
        "k\n",
        "​\n",
        " (o)\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "* If the LOF score of\n",
        "𝑝\n",
        "p is approximately 1, then\n",
        "𝑝\n",
        "p has a density similar to its neighbors and is likely a normal point. If the LOF score is significantly greater than 1, it indicates that\n",
        "𝑝\n",
        "p is in a sparser region than its neighbors, suggesting it is an anomaly. The higher the LOF score, the more anomalous the point is considered.\n",
        "# Interpreting the LOF Score\n",
        "* LOF ≈ 1: The point has a similar density to its neighbors, indicating it is likely part of a dense cluster and is not an anomaly.\n",
        "* LOF > 1: The point has a lower local density than its neighbors, suggesting it may be an anomaly. Higher values indicate stronger anomalies."
      ],
      "metadata": {
        "id": "2cK4Vlc3WHhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. What are the key parameters of the Isolation Forest algorithm?\n",
        "\n",
        "\n",
        "The Isolation Forest algorithm is an ensemble-based anomaly detection method that isolates data points by randomly partitioning the dataset. The algorithm relies on two main parameters that influence its performance and effectiveness in detecting anomalies:\n",
        "\n",
        "# **1. Number of Trees (n_estimators)**\n",
        "* This parameter specifies the number of isolation trees (also called \"iTrees\") in the forest. Each tree is built independently by randomly selecting features and splitting them at random values.\n",
        "* Effect: A higher number of trees generally leads to more accurate and stable anomaly scores, as it allows the model to capture a broader range of isolation patterns. However, increasing the number of trees also increases computational cost.\n",
        "* Typical Values: Commonly set between 100 and 200 for balanced accuracy and efficiency, but the optimal value depends on the dataset size and dimensionality.\n",
        "# **2. Subsample Size (max_samples)**\n",
        "* This parameter defines the sample size used to build each isolation tree. Instead of using the full dataset, Isolation Forest typically samples a subset of the data to increase diversity in each tree.\n",
        "* Effect: Smaller sample sizes allow the algorithm to build trees that can isolate anomalies more effectively, as they reduce the chance of anomalies blending in with larger clusters of normal points. Larger sample sizes increase computational cost but may improve accuracy if the dataset is large.\n",
        "* Typical Values: Commonly set to 256, as smaller samples are sufficient for isolation and anomalies are more likely to be detected in smaller subsets. However, for very large datasets, larger values may improve performance.\n",
        "# **3. Contamination (contamination)**\n",
        "* This parameter represents the expected proportion of anomalies in the dataset. It is used to determine the threshold for classifying a data point as an anomaly based on its anomaly score.\n",
        "* Effect: The contamination parameter helps the algorithm decide how many points to flag as anomalies. A correct setting improves accuracy, while an incorrect setting can lead to either too many false positives or false negatives.\n",
        "* Typical Values: This value is generally set based on prior knowledge of the dataset. For instance, if it's expected that around 1% of the data is anomalous, contamination should be set to 0.01. If unknown, it can be tuned or left at the default, in which case the algorithm internally sets a threshold.\n",
        "# **4. Tree Depth (Indirectly Determined)**\n",
        "* Tree depth in Isolation Forest is determined by the sample size, as each tree is grown until all points are isolated or the maximum depth is reached. A tree depth of\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "sample size\n",
        ")\n",
        "log\n",
        "2\n",
        "​\n",
        " (sample size) is typical.\n",
        "* Effect: Shallower trees (due to smaller samples) can isolate anomalies more efficiently, as anomalies tend to be isolated in fewer splits, resulting in shorter paths."
      ],
      "metadata": {
        "id": "tMhmvfofXFGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
        "using KNN with K=10?\n",
        "\n",
        "\n",
        "In the k-nearest neighbors (KNN) anomaly detection method with\n",
        "𝐾\n",
        "=\n",
        "10\n",
        "K=10, we typically calculate an anomaly score based on the distance to the\n",
        "𝐾\n",
        "K-nearest neighbors of each data point. However, given that this data point has only 2 neighbors of the same class within a radius of 0.5 and\n",
        "𝐾\n",
        "=\n",
        "10\n",
        "K=10, the point is quite isolated, indicating it could be an anomaly.\n",
        "\n",
        "Here are two common approaches for computing the anomaly score in KNN-based anomaly detection:\n",
        "\n",
        "1. **Distance-Based Score**:\n",
        "* The anomaly score can be the distance to the\n",
        "𝐾\n",
        "K-th nearest neighbor. Since the point only has 2 neighbors within the specified radius, and not the required 10 neighbors, this suggests the point is isolated. If the distance to the 10th neighbor is large, the anomaly score will be high, indicating a high likelihood of being an anomaly.\n",
        "2. **Density-Based Score (Relative Density)**:\n",
        "* Some KNN-based anomaly detection methods compute an anomaly score based on the density around the data point, comparing it to other points. Since this point has only 2 neighbors in a radius of 0.5, its local density would be low compared to other points with a full set of 10 close neighbors, yielding a higher anomaly score.\n",
        "# **Conclusion**\n",
        "\n",
        "In this case, the anomaly score would likely be high due to the lack of nearby neighbors. The exact score depends on the specific distance metric and scoring formula used, but with only 2 neighbors within the search radius (far fewer than the expected\n",
        "𝐾\n",
        "=\n",
        "10\n",
        "K=10), this point is likely to be classified as an anomaly.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8PfgPxFYXwj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
        "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
        "length of the trees?\n",
        "\n",
        "\n",
        "In the Isolation Forest algorithm, the anomaly score for a data point is calculated based on the average path length of that point across all isolation trees. Points that are isolated quickly (i.e., with shorter average path lengths) are considered more likely to be anomalies. Here’s how to calculate the anomaly score for a data point with an average path length of 5.0:\n",
        "\n",
        "# **Step 1: Calculate the Expected Average Path Length **\n",
        "𝑐\n",
        "(\n",
        "𝑛\n",
        ")\n",
        "c(n)\n",
        "The expected path length\n",
        "𝑐\n",
        "(\n",
        "𝑛\n",
        ")\n",
        "c(n) for a point in an isolation tree depends on the number of data points\n",
        "𝑛\n",
        "n in the dataset. For a dataset of size\n",
        "𝑛\n",
        "=\n",
        "3000\n",
        "n=3000, the expected average path length\n",
        "𝑐\n",
        "(\n",
        "𝑛\n",
        ")\n",
        "c(n) is given by:\n",
        "\n",
        "𝑐\n",
        "(\n",
        "𝑛\n",
        ")\n",
        "=\n",
        "2\n",
        "𝐻\n",
        "(\n",
        "𝑛\n",
        "−\n",
        "1\n",
        ")\n",
        "−\n",
        "2\n",
        "(\n",
        "𝑛\n",
        "−\n",
        "1\n",
        ")\n",
        "𝑛\n",
        "c(n)=2H(n−1)−\n",
        "n\n",
        "2(n−1)\n",
        "​\n",
        "\n",
        "where\n",
        "𝐻\n",
        "(\n",
        "𝑛\n",
        "−\n",
        "1\n",
        ")\n",
        "H(n−1) is the Harmonic number and can be approximated as\n",
        "𝐻\n",
        "(\n",
        "𝑛\n",
        "−\n",
        "1\n",
        ")\n",
        "≈\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "𝑛\n",
        "−\n",
        "1\n",
        ")\n",
        "+\n",
        "0.577215\n",
        "H(n−1)≈ln(n−1)+0.577215 (Euler’s constant).\n",
        "\n",
        "Using\n",
        "𝑛\n",
        "=\n",
        "3000\n",
        "n=3000:\n",
        "\n",
        "𝑐\n",
        "(\n",
        "3000\n",
        ")\n",
        "≈\n",
        "2\n",
        "×\n",
        "(\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "2999\n",
        ")\n",
        "+\n",
        "0.577215\n",
        ")\n",
        "−\n",
        "2\n",
        "×\n",
        "2999\n",
        "3000\n",
        "c(3000)≈2×(ln(2999)+0.577215)−\n",
        "3000\n",
        "2×2999\n",
        "​\n",
        " # **Step 2: Compute the Anomaly Score**\n",
        "\n",
        "The anomaly score for a data point is calculated as:\n",
        "\n",
        "score\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "2\n",
        "−\n",
        " path_length\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "𝑐\n",
        "(\n",
        "𝑛\n",
        ")\n",
        "score(x)=2\n",
        "−\n",
        "c(n)\n",
        "path_length(x)\n",
        "​\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "path_length\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "* path_length(x) is the average path length of the data point (5.0 in this case),\n",
        "𝑐\n",
        "(\n",
        "𝑛\n",
        ")\n",
        "c(n) is the expected path length for normal points.\n",
        "If this score is close to 1, the point is likely an anomaly; if it’s close to 0, it’s likely normal.\n",
        "\n",
        "Let’s plug in the values to calculate the score.\n",
        "\n",
        "The anomaly score for the data point with an average path length of 5.0 is approximately 0.80. Since this score is closer to 1 than to 0, it suggests that the point is more likely to be an anomaly than a typical data point. ​​\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7k-ylWpTYSJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gWzpq6hlVqPI"
      }
    }
  ]
}