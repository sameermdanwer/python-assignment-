{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCAitbkEcOb2TFHJX0+dh2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameermdanwer/python-assignment-/blob/main/Support_Vector_Machines_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
        "algorithms?\n",
        "\n",
        "In machine learning, polynomial functions and kernel functions have a close relationship, especially in the context of algorithms like Support Vector Machines (SVMs) and other kernel-based methods. Here‚Äôs how they connect:\n",
        "\n",
        "1. **Polynomial Functions as Kernels**: A kernel function is a method used to map the original data into a higher-dimensional space without explicitly computing the coordinates in that space. Polynomial functions are one type of kernel function. For example, the polynomial kernel function, defined as\n",
        "ùêæ\n",
        "(\n",
        "ùë•\n",
        ",\n",
        "ùë¶\n",
        ")\n",
        "=\n",
        "(\n",
        "ùë•\n",
        "‚ãÖ\n",
        "ùë¶\n",
        "+\n",
        "ùëê\n",
        ")\n",
        "ùëë\n",
        "K(x,y)=(x‚ãÖy+c)\n",
        "d\n",
        " , where\n",
        "ùëê\n",
        "c is a constant, and\n",
        "ùëë\n",
        "d is the degree of the polynomial, is commonly used in SVMs to enable them to learn nonlinear decision boundaries.\n",
        "\n",
        "2. **Feature Transformation via Polynomial Kernels**: Applying a polynomial kernel function in an SVM is equivalent to transforming the original features into polynomial combinations, which allows linear models to capture more complex patterns. For instance, a polynomial kernel of degree 2 maps a 2-dimensional input vector\n",
        "[\n",
        "ùë•\n",
        "1\n",
        ",\n",
        "ùë•\n",
        "2\n",
        "]\n",
        "[x\n",
        "1\n",
        "‚Äã\n",
        " ,x\n",
        "2\n",
        "‚Äã\n",
        " ] into the transformed feature space with terms like\n",
        "[\n",
        "ùë•\n",
        "1\n",
        "2\n",
        ",\n",
        "ùë•\n",
        "2\n",
        "2\n",
        ",\n",
        "ùë•\n",
        "1\n",
        "ùë•\n",
        "2\n",
        "]\n",
        "[x\n",
        "1\n",
        "2\n",
        "‚Äã\n",
        " ,x\n",
        "2\n",
        "2\n",
        "‚Äã\n",
        " ,x\n",
        "1\n",
        "‚Äã\n",
        " x\n",
        "2\n",
        "‚Äã\n",
        " ], enabling the model to fit complex decision boundaries.\n",
        "\n",
        "3. **Efficient Computation of Higher Dimensions**: Without a kernel, adding polynomial terms directly into a model requires explicit computation and storage of the expanded feature space. Kernel functions like the polynomial kernel, however, use an implicit method (known as the \"kernel trick\") to compute inner products in the transformed space directly, which reduces computational costs and memory requirements.\n",
        "\n",
        "4. **Hyperparameter Tuning**: The polynomial degree\n",
        "ùëë\n",
        "d and constant\n",
        "ùëê\n",
        "c in a polynomial kernel are hyperparameters that can be tuned to balance model complexity and performance. A higher degree\n",
        "ùëë\n",
        "d can allow the model to capture more complex relationships but may increase the risk of overfitting.\n",
        "\n",
        "In summary, polynomial functions serve as kernel functions that enable machine learning algorithms to capture nonlinear patterns by mapping data into higher-dimensional polynomial feature spaces. This capability is particularly useful for algorithms like SVMs that use the kernel trick to leverage these transformations efficiently."
      ],
      "metadata": {
        "id": "0W1BwvpQ_sLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
        "\n",
        "To implement an SVM with a polynomial kernel in Python using Scikit-learn, we can use the SVC class from sklearn.svm. Here‚Äôs a step-by-step guide:\n",
        "\n",
        "# 1. Import Required Libraries\n"
      ],
      "metadata": {
        "id": "yUokIfMxBUbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "xjOUvBp1BvWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Generate Sample Data"
      ],
      "metadata": {
        "id": "3fnfUJIeBxyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n"
      ],
      "metadata": {
        "id": "VCXjbKQ7B0-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Split the Data into Training and Testing Sets"
      ],
      "metadata": {
        "id": "K19MbJ09B4P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "DHzIYv4HB6tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Initialize the SVM Model with a Polynomial Kernel\n",
        "Specify the kernel as 'poly', and you can set the degree of the polynomial, the regularization parameter C, and the coef0 constant (which corresponds to the\n",
        "ùëê\n",
        "c in\n",
        "(\n",
        "ùë•\n",
        "‚ãÖ\n",
        "ùë¶\n",
        "+\n",
        "ùëê\n",
        ")\n",
        "ùëë\n",
        "(x‚ãÖy+c)\n",
        "d\n",
        " )."
      ],
      "metadata": {
        "id": "SRFZdy7_B9Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SVM model with a polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1, coef0=1)\n"
      ],
      "metadata": {
        "id": "R2bvbqE9CFGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Train the Model"
      ],
      "metadata": {
        "id": "iSBGxPluCJmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "svm_poly.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "EJcw5FnvCMZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Make Predictions"
      ],
      "metadata": {
        "id": "aJsk0dpBCOnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred = svm_poly.predict(X_test)\n"
      ],
      "metadata": {
        "id": "DifZHhckCSGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Evaluate the Model"
      ],
      "metadata": {
        "id": "qu0uuj_VCUxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy with Polynomial Kernel SVM:\", accuracy)\n"
      ],
      "metadata": {
        "id": "I2HizT7oCX2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM model with polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1, coef0=1)\n",
        "\n",
        "# Train the model\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_poly.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy with Polynomial Kernel SVM:\", accuracy)\n"
      ],
      "metadata": {
        "id": "wmtbMu3DCa34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
        "\n",
        "In Support Vector Regression (SVR), the epsilon (Œµ) parameter controls the width of the margin (also called the epsilon-tube) within which no penalty is given to errors. Essentially, epsilon defines a zone around the regression line where errors are considered negligible, and points within this zone are ignored when defining the model.\n",
        "\n",
        "When we increase the value of epsilon:\n",
        "\n",
        "1. **Wider Margin (Epsilon-Tube)**:\n",
        "\n",
        "The margin around the predicted line becomes wider, allowing for a larger region where deviations between predicted and actual values are ignored.\n",
        "2. **Fewer Support Vectors**:\n",
        "\n",
        "* With a larger epsilon, more data points fall within the epsilon-tube, and these points are not considered as support vectors. This leads to a reduction in the number of support vectors needed to define the model.\n",
        "* Only the points outside this wider margin contribute to the SVR model, as they are the only ones with a non-zero error that SVR tries to minimize.\n",
        "3. **Increased Model Sparsity**:\n",
        "\n",
        "Since fewer support vectors are involved, the model becomes sparser, which may improve computational efficiency.\n",
        "4. **Potential Loss of Model Sensitivity**:\n",
        "\n",
        "* With fewer support vectors, the model becomes less sensitive to small fluctuations in data. This can be beneficial for reducing overfitting, but if epsilon is too large, the model might fail to capture finer patterns, reducing prediction accuracy.\n",
        "\n",
        "In summary, increasing epsilon generally decreases the number of support vectors in SVR, leading to a sparser model with reduced sensitivity to small errors but potentially a loss in accuracy if set too high."
      ],
      "metadata": {
        "id": "pMZAFO_ZCcAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
        "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
        "and provide examples of when you might want to increase or decrease its value?\n",
        "\n",
        "\n",
        "In Support Vector Regression (SVR), the choice of kernel function, and the values of\n",
        "ùê∂\n",
        "C, epsilon (Œµ), and gamma parameters have a significant impact on model performance. Each parameter influences the SVR model's behavior in unique ways. Here‚Äôs a breakdown of each parameter and guidance on when to increase or decrease their values:\n",
        "\n",
        "# 1. Kernel Function\n",
        "The kernel function determines how data is transformed or mapped to a higher-dimensional space to capture nonlinear relationships.\n",
        "\n",
        "* **Common Kernels**:\n",
        "* **Linear Kernel**: Suitable for linear relationships; faster and less complex.\n",
        "* **Polynomial Kernel**: Captures polynomial relationships; more flexible but can be computationally intensive.\n",
        "* **RBF (Radial Basis Function) Kernel**: Captures complex, nonlinear relationships; often effective for general nonlinear patterns.\n",
        "* When to Use Each:\n",
        "* **Linear Kernel**: Use when data has a roughly linear trend. It‚Äôs also faster and less prone to overfitting in high-dimensional spaces.\n",
        "* **Polynomial/RBF Kernel**: Use when the data shows more complex, nonlinear relationships. The RBF kernel is often the default choice because it handles a broad range of patterns.\n",
        "\n",
        "# 2. C Parameter (Regularization)\n",
        "The C parameter controls the trade-off between maximizing the margin and minimizing the error. A higher\n",
        "ùê∂\n",
        "C value makes the model focus on minimizing errors, whereas a lower\n",
        "ùê∂\n",
        "C value emphasizes a larger margin.\n",
        "\n",
        "* **Effects of C**:\n",
        "\n",
        "* **High C**: Reduces tolerance for errors, making the model fit the data more closely, potentially at the risk of overfitting.\n",
        "* **Low C**: Increases tolerance for errors, creating a softer margin and reducing the likelihood of overfitting but potentially underfitting.\n",
        "**When to Adjust C**:\n",
        "\n",
        "**Increase C** if the model is underfitting and isn‚Äôt capturing important trends in the data.\n",
        "**Decrease C** if the model is overfitting and capturing too much noise from the training data.\n",
        "\n",
        "# 3. Epsilon (Œµ Parameter)\n",
        "The epsilon parameter (Œµ) defines the width of the ‚Äúepsilon-tube‚Äù around the regression line, within which errors are ignored.\n",
        "\n",
        "* **Effects of Epsilon**:\n",
        "\n",
        "* **High Epsilon**: Wider margin around the regression line, allowing more points to fall within the margin without contributing to the loss function. This reduces the number of support vectors and makes the model less sensitive to small fluctuations.\n",
        "* **Low Epsilon**: Narrower margin, requiring the model to fit the data more closely, resulting in more support vectors.\n",
        "* **When to Adjust Epsilon**:\n",
        "\n",
        "**Increas** Œµ if the model is overfitting and sensitive to minor noise in the data.\n",
        "**Decrease** Œµ if the model is underfitting and missing important details.\n",
        "\n",
        "# 4. Gamma Parameter (For RBF and Polynomial Kernels)\n",
        "The gamma parameter defines the influence of individual data points in the RBF and polynomial kernels. It controls the ‚Äúspread‚Äù of data points within the kernel space.\n",
        "\n",
        "* **Effects of Gamma**:\n",
        "\n",
        "* **High Gamma**: Each point has a very narrow area of influence, causing the model to capture more detail in the training data, which can lead to overfitting.\n",
        "* **Low Gamma**: Each point has a broader influence, which smooths out the decision boundary and may cause underfitting if set too low.\n",
        "* When to Adjust Gamma:\n",
        "\n",
        "* **Increase Gamma** if the model is underfitting and struggling to capture complex patterns in the data.\n",
        "* **Decrease Gamma** if the model is overfitting and capturing too much noise or minor details.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j53Q8QA6DnxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Assignment:\n",
        "L Import the necessary libraries and load the dataseg\n",
        "L Split the dataset into training and testing setZ\n",
        "L Preprocess the data using any technique of your choice (e.g. scaling, normalizationK\n",
        "L Create an instance of the SVC classifier and train it on the training datW\n",
        "L Use the trained classifier to predict the labels of the testing datW\n",
        "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
        "precision, recall, F1-scoreK\n",
        "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
        "improve its performanc_\n",
        "L Train the tuned classifier on the entire dataseg\n",
        "L Save the trained classifier to a file for future use.\n",
        "\n",
        "Here is a step-by-step solution for the assignment, demonstrating how to load data, preprocess it, train and evaluate an SVM classifier, perform hyperparameter tuning, and save the trained model.\n",
        "\n",
        "Let's proceed with a commonly used dataset in machine learning, like the Iris dataset. You can replace this with your own dataset if needed.\n",
        "\n",
        "# Step-by-Step Solution:\n"
      ],
      "metadata": {
        "id": "hgTkTfA-GHtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import the Necessary Libraries and Load the Dataset"
      ],
      "metadata": {
        "id": "-X-VppSEGnAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Load the dataset (e.g., Iris dataset)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n"
      ],
      "metadata": {
        "id": "mFPgzKsLGq6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Split the Dataset into Training and Testing Sets"
      ],
      "metadata": {
        "id": "W8DIao_LGsk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "GNxRLYqbGvUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Preprocess the Data (Scaling)"
      ],
      "metadata": {
        "id": "7svShUdhGzMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the scaler and fit it on the training data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "W5j8bhrqG0sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Create an Instance of the SVC Classifier and Train It on the Training Data"
      ],
      "metadata": {
        "id": "oz6rHPIpG3g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the SVC classifier with default parameters\n",
        "svc = SVC()\n",
        "svc.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "NSg5jq77G7ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Use the Trained Classifier to Predict the Labels of the Testing Data"
      ],
      "metadata": {
        "id": "SI_n2IsyG-Lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict labels for the test set\n",
        "y_pred = svc.predict(X_test)\n"
      ],
      "metadata": {
        "id": "Mibo3kOmHEnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluate the Performance of the Classifier\n",
        "Using accuracy, precision, recall, and F1-score as evaluation metrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "lwVnFxxDHId6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "HuGmrsVeHKK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Tune the Hyperparameters of the SVC Classifier Using GridSearchCV\n",
        "We‚Äôll tune C, kernel, and gamma parameters to improve performance"
      ],
      "metadata": {
        "id": "U_bcUYKSHL4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'poly', 'rbf'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters found\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "H43Tzt8_HQMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Train the Tuned Classifier on the Entire Dataset\n",
        "Use the best parameters from GridSearchCV to train the classifier on the entire dataset."
      ],
      "metadata": {
        "id": "2DMkZM0OHTwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train SVC with best parameters on the entire dataset\n",
        "best_svc = grid_search.best_estimator_\n",
        "best_svc.fit(X, y)\n"
      ],
      "metadata": {
        "id": "-ZLBMaT8HVab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Save the Trained Classifier to a File for Future Use\n",
        "Using joblib to save the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Bm_9YTCHX2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to a file\n",
        "joblib.dump(best_svc, 'svm_classifier_model.joblib')\n",
        "print(\"Model saved as 'svm_classifier_model.joblib'\")\n"
      ],
      "metadata": {
        "id": "ayaME2YlHcwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Code Summary"
      ],
      "metadata": {
        "id": "Sy_rDDpbHhPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the SVC\n",
        "svc = SVC()\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Tune hyperparameters using GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'poly', 'rbf'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Train the best model on the entire dataset\n",
        "best_svc = grid_search.best_estimator_\n",
        "best_svc.fit(X, y)\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(best_svc, 'svm_classifier_model.joblib')\n",
        "print(\"Model saved as 'svm_classifier_model.joblib'\")\n"
      ],
      "metadata": {
        "id": "6oBpd2qqHihK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}